{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hrKJa873t8-I"
      },
      "source": [
        "# Unsupervised Domain Adaptation \n",
        "\n",
        "<center>\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bonom/DL-Project/blob/main/notebook.ipynb)\n",
        "\n",
        "</center>\n",
        "\n",
        "Notebook for the final project of the course \"Machine Learning\" at the University of Trento.\n",
        "\n",
        "## Students:\n",
        "- Andrea Bonomi [GitHub](https://github.com/bonom)\n",
        "- Evelyn Turri [GitHub](https://github.com/EvelynTurri)\n",
        "\n",
        "## Weights\n",
        "\n",
        "Al the weights of the best models we trained are available [here](https://drive.google.com/drive/folders/1YNAXZ96Deq58423yJ9ignHHTZMdbl33n?usp=share_link)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zYfLXQ3auTwA"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this notebook, we will explore the concept of **domain adaptation** and how it can be applied to improve the performance of machine learning models. In domain adaptation we aim at learning from a source data distribution a well performing model on a different (but related) target data distribution. This is particularly useful when we have limited labeled data in the target domain but a large amount of labeled data in the source domain.\n",
        "\n",
        "We will start by discussing the problem of domain shift and how it can affect the performance of a model. Then, we will introduce different domain adaptation techniques such as feature alignment and adversarial training. We will also implement these techniques in PyTorch and evaluate their effectiveness on a real-world dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal\n",
        "\n",
        "The goal is to implement an UDA technique for the problem of multiclass classification task with a suibset of the [Adaptiope](https://ieeexplore.ieee.org/document/9423412) dataset, consisting of images belonging to two different domains: Real Life and Product. The UDA technique can be chosen between:\n",
        "\n",
        " - Discrepancy-based methods\n",
        " - Adversarial-based methods\n",
        " - Reconstruction-based methods\n",
        "\n",
        "As UDA method we present the **Maximum Classifier Discrepancy** (MCD) approach, a discrepancy-based one. This method is taken from the paper [Maximum Classifier Discrepancy for Unsupervised Domain Adapation](https://openaccess.thecvf.com/content_cvpr_2018/papers/Saito_Maximum_Classifier_Discrepancy_CVPR_2018_paper.pdf). And we will show the results obtained with this method and some possible improvements."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Organization & Lineguides\n",
        "\n",
        "Below we present the organization of the notebook and the lineguides that we followed during the project.\n",
        "\n",
        "### Notations\n",
        "\n",
        "- Source domain = domain from which we have the labeled data\n",
        "- Target domain = domain to which we want to adapt our model (we have no labeled data)\n",
        "- $P$ = Product domain\n",
        "- $RW$ = Real World domain\n",
        "- $P \\rightarrow RW$ = Product to Real World: source domain is $P$ and target domain is $RW$\n",
        "- $RW \\rightarrow P$ = Real World to Product: source domain is $RW$ and target domain is $P$\n",
        "\n",
        "### Dataset\n",
        "\n",
        "Consider that datasets are split as following:\n",
        "\n",
        "- Source domain = Product domain:\n",
        "    - Train = $P_{train}$\n",
        "    - Test = $P_{test}$\n",
        "- Target domain = Real World domain:\n",
        "    - Train = $RW_{train}$\n",
        "    - Test = $RW_{test}$\n",
        "  \n",
        "### Tasks\n",
        "\n",
        "Task that we accomplish, for both directions ($P$ as source domain and $RW$ as target domain and viceversa):\n",
        "\n",
        "1. *Baseline* \n",
        "    - Train the proposed model supervisedly on the source domain and evaluate it as it is on the target\n",
        "    - Accuracy here is called $acc_{so}$ referred to the *source only* scenario\n",
        "    - Finally, we compute the baseline in both directions: $P \\rightarrow RW$ and $RW \\rightarrow P$\n",
        "\n",
        "2. *Upperbound* \n",
        "    - Train supervisedely on the target train set and test on the target test set\n",
        "        - $P \\rightarrow RW$ : train on $RW_{train}$ and test on $RW_{test}$\n",
        "        - $RW \\rightarrow P$ : train on $P_{train}$ and test on $P_{test}$\n",
        "\n",
        "3. *Advanced* \n",
        "    - Simultaneously:\n",
        "        - Train supervisedly on the source domain\n",
        "        - Train unsupervisedly on the target domain applying the UDA component\n",
        "    - Test on the target domain\n",
        "    - Accuracy here is called $acc_{uda}$ referred to the *unsupervised domain adaptation* scenario  \n",
        "    - Compute the gain: $G = acc_{uda} - acc_{so}$\n",
        "  \n",
        "        > Notice that $acc_{uda}$ should be bigger than $acc_{so}$ \n",
        "\n",
        "4. *Our proposals*\n",
        "    - Try to improve the results of the advanced task with the help of some papers and ideas that we will present in the report\n",
        "    - Compute the gain: $G = acc_{uda} - acc_{so}$\n",
        "### Accuracies assignment\n",
        "\n",
        "Minimum accuracy required for each task:\n",
        "\n",
        "<center>\n",
        "\n",
        "| Version | $P \\rightarrow RW$ | $RW \\rightarrow P$ |\n",
        "| :---: | :---: | :---: |\n",
        "| Source only | 76% | 90% |\n",
        "| DA | 80% | 93% |\n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0jKgHmT3Ln8"
      },
      "source": [
        "## Preparing the Notebook"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lqddvBoq3Ln8"
      },
      "source": [
        "### Requirements\n",
        "\n",
        "- Google Colab (pre-installed)\n",
        "- wandb (to install: `!pip install -U wandb`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAU3cHmX3Ln9"
      },
      "outputs": [],
      "source": [
        "! pip install -qU wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZajpQnwqPJt9"
      },
      "outputs": [],
      "source": [
        "# For plotting correctly\n",
        "%matplotlib inline "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8wFX1Lw3Ln9"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpGAamNsIDKM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import time\n",
        "import shutil\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple, List, Dict, Union, Optional, Callable\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "\n",
        "# If on google colab needs to import drive to mount google drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "except ImportError:\n",
        "    print(\"Not using Google Colab, not mounting google drive.\")\n",
        "\n",
        "# To avoid errors with wandb\n",
        "os.environ['WANDB_NOTEBOOK_NAME'] = '229357_230616.ipynb'\n",
        "\n",
        "import wandb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wandb setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZLWZZbV3Ln-",
        "outputId": "fd8df04f-ac0c-40bc-8c88-6d21dbdc4b96"
      },
      "outputs": [],
      "source": [
        "if wandb.login():\n",
        "    print(\"Logged in to Weights & Biases!\")\n",
        "else:\n",
        "    if wandb.login(relogin=True):\n",
        "        print(\"Logged in to Weights & Biases!\")\n",
        "    else:\n",
        "        raise Exception(\"Could not log in to Weights & Biases!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting\n",
        "\n",
        "For a best view of the `matplotlib` plots in the notebook we use code explained in this [Github Gist](https://gist.github.com/micaleel/965d1672a58af2fcd469d8a24024cb29)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.core.display import HTML\n",
        "\n",
        "HTML(\"\"\"\n",
        "<style>\n",
        " {\n",
        "    display: table-cell;\n",
        "    text-align: center;\n",
        "   .output_png vertical-align: middle;\n",
        "}\n",
        "</style>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX6ZRFJsNWqW"
      },
      "source": [
        "### Constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZovKvdr3BY3",
        "outputId": "305f3d24-6a71-48c0-9942-a5f0c850ab3f"
      },
      "outputs": [],
      "source": [
        "# For reproducibility, set the seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# To fix the width of the pandas dataframes\n",
        "pd.set_option('expand_frame_repr', False)\n",
        "\n",
        "# Device configuration\n",
        "DEFAULT_DEVICE = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using GPU as default device!\")\n",
        "    DEFAULT_DEVICE = torch.device(\"cuda\")\n",
        "    torch.backends.cudnn.enabled = True # Accelerate cuda\n",
        "else:\n",
        "    print(\"Using CPU as default device!\\nThe performances will decrease significantly!\")\n",
        "\n",
        "# Hyper-parameters\n",
        "DEFAULT_BATCH_SIZE = 256\n",
        "DEFAULT_CLASSES = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\", \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\", \"purse\", \"stand mixer\", \"stroller\"]\n",
        "DEFAULT_EPOCHS_BASELINE = 10\n",
        "DEFAULT_EPOCHS = 30\n",
        "DEFAULT_GAMMA = 0.1\n",
        "DEFAULT_GENERATOR_SPLIT = torch.Generator().manual_seed(0)\n",
        "DEFAULT_K_STEPS = 4\n",
        "DEFAULT_LEARNING_RATE = 1e-1\n",
        "DEFAULT_MOMENTUM = 0.9\n",
        "DEFAULT_NUM_CLASSES = len(DEFAULT_CLASSES)\n",
        "DEFAULT_SHUFFLE_DATALOADER = True\n",
        "DEFAULT_SPLIT_RATIO = 0.8\n",
        "DEFAULT_STEP_SIZE = 1\n",
        "DEFAULT_TSNE_PERPLEXITY = 5\n",
        "DEFAULT_WANDB_PROJECT_NAME = \"229357_230616\"\n",
        "DEFAULT_WEIGHT_DECAY = 1e-5\n",
        "\n",
        "### Paths for saving models\n",
        "# First get the time and date\n",
        "_time_now = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Then create the path\n",
        "LOG_PATH = os.path.join(\"logs\", _time_now)\n",
        "LOG_PATH_MODELS = os.path.join(LOG_PATH, \"models\")\n",
        "LOG_PATH_IMAGES = os.path.join(LOG_PATH, \"images\")\n",
        "\n",
        "# Create the directories\n",
        "if not os.path.exists(LOG_PATH_MODELS):\n",
        "    os.makedirs(LOG_PATH_MODELS)\n",
        "if not os.path.exists(LOG_PATH_IMAGES):\n",
        "    os.makedirs(LOG_PATH_IMAGES)\n",
        "\n",
        "# Release variables that are no longer needed\n",
        "del _time_now "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hP7wHlbrNFKO"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The provided dataset is [Adaptiope](https://ieeexplore.ieee.org/document/9423412). We use only a subset of the dataset, which is composed by 2000 images for each domain. Indeed, we have two different domains: Real World and Product. The images are divided in 20 classes, as follows: `backpack`, `bookcase`, `car jack`, `comb`, `crown`, `file cabinet`, `flat iron`, `game controller`, `glasses`, `helicopter`, `ice skates`, `letter tray`, `monitor`, `mug`, `network switch`, `over-ear headphones`, `pen`, `purse`, `stand mixer`, `stroller`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5hvcP2sI3LoA"
      },
      "source": [
        "### ONLY IF you have already unzipted the dataset\n",
        "If you have already downloaded the unzipted dataset, you can just run this cell to load the dataset from the drive. Otherwise skip this cell and go to the next one.\n",
        "\n",
        "Note that the dataset should be in the folder `dataset` in your personal drive. Otherwise change `gdrive_path` accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHmG3dv13LoA"
      },
      "outputs": [],
      "source": [
        "gdrive_path = \"gdrive/My Drive/dataset/adaptiope_small\"\n",
        "if not os.path.exists(gdrive_path):\n",
        "    # Mount google drive if not already mounted\n",
        "    if not os.path.exists(\"gdrive\"):\n",
        "        drive.mount('/content/gdrive')\n",
        "\n",
        "    # To load the data from google drive faster it is possible to copy directly the extracted dataset\n",
        "    classes = DEFAULT_CLASSES\n",
        "    if os.path.isdir(gdrive_path):\n",
        "        for d, td in zip([os.path.join(gdrive_path, \"product_images\"), os.path.join(gdrive_path, \"real_life\")], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "            os.makedirs(td, exist_ok=True)\n",
        "            for c in tqdm(classes):\n",
        "                c_path = os.path.join(d, c)\n",
        "                c_target = os.path.join(td, c)\n",
        "                shutil.copytree(c_path, c_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpOd89Zy3Ln_"
      },
      "source": [
        "### Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egggXBVBOCTB",
        "outputId": "d9dfcda8-03a1-40b8-d38d-76fe218404f0"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7e8E82z3Ln_"
      },
      "outputs": [],
      "source": [
        "! mkdir dataset\n",
        "! cp \"gdrive/My Drive/dataset/Adaptiope.zip\" dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPl4L0Xu3LoA"
      },
      "outputs": [],
      "source": [
        "! unzip -qq dataset/Adaptiope.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAIPHzM63LoA"
      },
      "outputs": [],
      "source": [
        "# If there is an old version of the dataset it is better to remove it for safety\n",
        "! rm -rf adaptiope_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h_JRKtKPJuD"
      },
      "outputs": [],
      "source": [
        "# Create folder\n",
        "! mkdir adaptiope_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDiq0L8C3LoA",
        "outputId": "a72393d9-d1de-4a97-8096-68cdd5f502f5"
      },
      "outputs": [],
      "source": [
        "# Build the dataset folder\n",
        "classes = DEFAULT_CLASSES\n",
        "for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "    os.makedirs(td)\n",
        "    for c in tqdm(classes):\n",
        "        c_path = os.path.join(d, c)\n",
        "        c_target = os.path.join(td, c)\n",
        "        shutil.copytree(c_path, c_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctnCzfUdOMVY"
      },
      "source": [
        "### Transformations\n",
        "Transformations on the input (as said in the assignment instructions). For the transformations we use the `torchvision.transforms` library. We apply the same transformations to both domains. \n",
        "1. Resize the image to `224x224` pixels to be compatible with the Resnet architecture\n",
        "2. Convert the image to a tensor to avoid errors\n",
        "3. Normalize the image with the mean and standard deviation of the ImageNet dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv57pLTQqnpw"
      },
      "outputs": [],
      "source": [
        "transformations = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                          # Resize to the input size of resnet\n",
        "    transforms.ToTensor(),                                  # Convert to pytorch Tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],        # Normalize with ImageNet mean and std\n",
        "                         std=[0.229, 0.224, 0.225])   \n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eeWyIVAMPJuE"
      },
      "source": [
        "### Load Datasets\n",
        "\n",
        "This cell loads two datasets, *product_images* and *real_life* from the *adaptiope_small* directory and applies the same set of `transformations` to each of them. \n",
        "\n",
        "It checks if the default `device` is CPU too: if so, it reduces the batch size to 8 and reduces the size of both datasets to 100 samples each to avoid long training times. This is done with the help of the `torch.utils.data.Subset` utility class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUMEp0z6PJuE",
        "outputId": "64488e37-4a01-4233-8ac0-4b2ebd174ab2"
      },
      "outputs": [],
      "source": [
        "# Load datasets and apply transformations\n",
        "products_dataset = torchvision.datasets.ImageFolder('adaptiope_small/product_images', transformations)\n",
        "reals_dataset = torchvision.datasets.ImageFolder('adaptiope_small/real_life', transformations)\n",
        "\n",
        "# If using CPU it is better to use smaller datasets to avoid long training times\n",
        "if DEFAULT_DEVICE == torch.device(\"cpu\"):\n",
        "    DEFAULT_BATCH_SIZE = 8\n",
        "    products_dataset = Subset(products_dataset, range(0, 100))\n",
        "    reals_dataset = Subset(reals_dataset, range(0, 100))\n",
        "\n",
        "print(\"Products dataset size: {}\".format(len(products_dataset)))\n",
        "print(\"Real life dataset size: {}\".format(len(reals_dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uYGAeMfPJuE"
      },
      "source": [
        "### Create Train and Test Datasets\n",
        "\n",
        "Both datasets are splitted in train and test set with a ratio of `DEFAULT_SPLIT_RATIO` (Default: 80% and 20%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa9M5wBWPJuE",
        "outputId": "eacd7a9a-e89a-4da5-fcba-cef818ac1de4"
      },
      "outputs": [],
      "source": [
        "# Split dataset into train and test\n",
        "split_product_train = int(len(products_dataset)*DEFAULT_SPLIT_RATIO)\n",
        "split_real_train = int(len(reals_dataset)*DEFAULT_SPLIT_RATIO)\n",
        "\n",
        "print(\"Splitting products dataset into train and test with ratio {}:\\n\\n\\tTrain product = {}\\n\\tTest product = {}\\n\\n\\tTrain real life = {}\\n\\tTest real life = {}\".format(\n",
        "    DEFAULT_SPLIT_RATIO, \n",
        "    split_product_train, \n",
        "    len(products_dataset)-split_product_train, \n",
        "    split_real_train, \n",
        "    len(reals_dataset)-split_real_train)\n",
        ")\n",
        "\n",
        "# Split dataset into train and test\n",
        "train_products, test_products = random_split(products_dataset, [split_product_train, len(products_dataset)-split_product_train], generator = DEFAULT_GENERATOR_SPLIT)\n",
        "train_reals, test_reals = random_split(reals_dataset, [split_real_train, len(reals_dataset)-split_real_train], generator = DEFAULT_GENERATOR_SPLIT)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cE7528ns3LoB"
      },
      "source": [
        "### Create Dataloaders\n",
        "\n",
        "Build the dataloaders for both domains. We enable the `shuffle` parameter to shuffle the dataset and improve the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE7UfHKVrZLU"
      },
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "train_loader_products = DataLoader(train_products, batch_size=DEFAULT_BATCH_SIZE, shuffle=DEFAULT_SHUFFLE_DATALOADER)\n",
        "test_loader_products = DataLoader(test_products, batch_size=DEFAULT_BATCH_SIZE, shuffle=DEFAULT_SHUFFLE_DATALOADER)\n",
        "train_loader_reals = DataLoader(train_reals, batch_size=DEFAULT_BATCH_SIZE, shuffle=DEFAULT_SHUFFLE_DATALOADER)\n",
        "test_loader_reals = DataLoader(test_reals, batch_size=DEFAULT_BATCH_SIZE, shuffle=DEFAULT_SHUFFLE_DATALOADER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mE1wjGx3LoB"
      },
      "source": [
        "## Global Functions\n",
        "\n",
        "Definition of the most used functions during the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v5sM23GPJuF"
      },
      "source": [
        "### Accuracy function\n",
        "Definition of the accuracy function as defined from the instructions: $$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$ \n",
        "where:\n",
        "- TP = True Positives\n",
        "- TN = True Negatives\n",
        "- FP = False Positives\n",
        "- FN = False Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hiOx5vt3LoB"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(prediction: torch.Tensor, label: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Compute the accuracy of a prediction given the label.\"\"\"\n",
        "    # Compute accuracy\n",
        "    _, predicted = prediction.max(dim=1)\n",
        "\n",
        "    return (predicted == label).sum().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHkRQ3Xj3LoB"
      },
      "source": [
        "### Gain function\n",
        "\n",
        "Definition of the **gain**: $$ G = acc_{uda} - acc_{so} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P7c1xay3LoB"
      },
      "outputs": [],
      "source": [
        "def compute_gain(acc_uda, acc_so) -> float:\n",
        "    return acc_uda - acc_so"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "InULir2l3LoC"
      },
      "source": [
        "### Logger function\n",
        "\n",
        "Function to log the results in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnZfWCgF3LoC"
      },
      "outputs": [],
      "source": [
        "def make_log_print(status:str = \"Train\", epoch:tuple = None, timer:float = None, metrics: dict = None, *args, **kwargs) -> None:\n",
        "    def _convert_seconds_to_h_m_s(seconds):\n",
        "        m, s = divmod(seconds, 60)\n",
        "        h, m = divmod(m, 60)\n",
        "        h, m, s = int(h), int(m), int(s)\n",
        "\n",
        "        if h > 0:\n",
        "            _ret = f\"{h}:{m}:{s} hh:mm:ss\"\n",
        "        else:\n",
        "            if m > 0:\n",
        "                _ret = f\"{m}:{s} mm:ss\"\n",
        "            else:\n",
        "                _ret = f\"{s} seconds\"\n",
        "          \n",
        "        return _ret\n",
        "    \n",
        "    _string = \" \" + status + \" \"\n",
        "    if epoch is not None:\n",
        "        _actual_epoch = epoch[0]\n",
        "        _total_epoch = epoch[1]\n",
        "\n",
        "        _string = \" \" + status + \" - Epoch \" + str(_actual_epoch) + \"/\" + str(_total_epoch) + \" \" \n",
        "      \n",
        "    # Convert seconds of timer to minutes:seconds\n",
        "    if timer is not None:\n",
        "        _chrono = _convert_seconds_to_h_m_s(timer)\n",
        "        \n",
        "        # Compute the estimate time remaining \n",
        "        _eta = (_total_epoch - _actual_epoch) * timer/_actual_epoch \n",
        "        _eta = _convert_seconds_to_h_m_s(_eta)\n",
        "\n",
        "    \n",
        "    print(f\"{_string:=^50}\")\n",
        "    if metrics is not None:\n",
        "        if \"train/loss_source\" in metrics.keys():\n",
        "            print(f\"  Training loss source {metrics['train/loss_source']:.3f}, Training accuracy source {metrics['train/accuracy_source']:.3f}\")\n",
        "            print(f\"  Training loss target {metrics['train/loss_target']:.3f}, Training accuracy target {metrics['train/accuracy_target']:.3f}\")\n",
        "            print(f\"  Discrepancy {metrics['train/discrepancy']:.3f}, Discrepancy generator {metrics['train/loss_features']:.3f}\")\n",
        "        elif \"train/loss\" in metrics.keys():\n",
        "            print(f\"  Training loss {metrics['train/loss']:.3f}, Training accuracy {metrics['train/accuracy']:.3f}\")\n",
        "        if \"eval/accuracy_source\" in metrics.keys():\n",
        "            print(f\"  Test loss {metrics['eval/loss']:.3f}, Test accuracy source {metrics['eval/accuracy_source']:.3f}, Test accuracy target {metrics['eval/accuracy_target']:.3f}, Test accuracy total {metrics['eval/accuracy']:.3f}\")\n",
        "        elif \"eval/loss\" in metrics.keys():\n",
        "            print(f\"  Test loss {metrics['eval/loss']:.3f}, Test accuracy {metrics['eval/accuracy']:.3f}\")\n",
        "\n",
        "    if timer is not None and epoch is not None:\n",
        "        print(f\"  Time/epoch: {timer/_actual_epoch:.2f} s - Time elapsed: {_chrono} - Time remaining: {_eta}\")\n",
        "\n",
        "    if args is not None:\n",
        "        for arg in args:\n",
        "            for k, v in arg.items():\n",
        "                print(f\"  {k}: {v}\")\n",
        "\n",
        "    if kwargs is not None:\n",
        "        for k, v in kwargs.items():\n",
        "            print(f\"  {k}: {v}\")\n",
        "      \n",
        "    _string = \" End \" + status.lower() + \" \"\n",
        "    if epoch is not None:\n",
        "        _string = \" End \" + status.lower() + \" \" + str(_actual_epoch) + \"/\" + str(_total_epoch) + \" \"\n",
        "\n",
        "    print(f\"{_string:=^50}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YFd9a7tg3LoC"
      },
      "source": [
        "### Optimizer, Criterion and Scheduler\n",
        "\n",
        "These three functions are used to create the optimizer, the criterion and the scheduler. It is possible to change the parameters by adding them to the function call.\n",
        "\n",
        "#### Example\n",
        "\n",
        "```python\n",
        "optimizer = get_optimizer(model, optimizer='sgd', lr=0.001, momentum=0.9)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRSrGyTX3LoC"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model: nn.Module, optimizer: str = 'adam', *args, **kwargs) -> torch.optim.Optimizer:\n",
        "    _lr = DEFAULT_LEARNING_RATE\n",
        "    _weight_decay = DEFAULT_WEIGHT_DECAY \n",
        "    _momentum = DEFAULT_MOMENTUM\n",
        "    if \"lr\" in kwargs:\n",
        "        _lr = kwargs[\"lr\"]\n",
        "    if \"weight_decay\" in kwargs:\n",
        "        _weight_decay = kwargs[\"weight_decay\"]\n",
        "\n",
        "    if optimizer == 'adam':\n",
        "        return torch.optim.Adam(model.parameters(), lr=_lr, weight_decay=_weight_decay)\n",
        "    elif optimizer == 'sgd':\n",
        "        if \"momentum\" in kwargs:\n",
        "            _momentum = kwargs[\"momentum\"]\n",
        "        return torch.optim.SGD(model.parameters(), lr=_lr, momentum=_momentum, weight_decay=_weight_decay)\n",
        "    else:\n",
        "        raise Exception(f\"Optimizer {optimizer} not implemented.\")\n",
        "\n",
        "def get_criterion(criterion: str = 'cross_entropy', *args, **kwargs) -> nn.Module:\n",
        "    if criterion == 'cross_entropy':\n",
        "        return nn.CrossEntropyLoss()\n",
        "    elif criterion == 'nll':\n",
        "        return nn.NLLLoss()\n",
        "    else:\n",
        "        raise Exception(f\"Criterion {criterion} not implemented.\")\n",
        "\n",
        "def get_scheduler(optimizer: torch.optim.Optimizer, scheduler: str = 'lambda', *args, **kwargs) -> torch.optim.lr_scheduler._LRScheduler:\n",
        "    _lr_lambda = lambda epoch: 0.9 ** epoch\n",
        "    _step_size = DEFAULT_STEP_SIZE\n",
        "    _gamma = DEFAULT_GAMMA\n",
        "\n",
        "    if \"lr_lambda\" in kwargs:\n",
        "        _lr_lambda = kwargs[\"lr_lambda\"]\n",
        "    if \"step_size\" in kwargs:\n",
        "        _step_size = kwargs[\"step_size\"]\n",
        "    if \"gamma\" in kwargs:\n",
        "        _gamma = kwargs[\"gamma\"]\n",
        "\n",
        "    if scheduler == 'lambda':\n",
        "        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=_lr_lambda)\n",
        "    elif scheduler == 'step':\n",
        "        return torch.optim.lr_scheduler.StepLR(optimizer, step_size=_step_size, gamma=_gamma)\n",
        "    else:\n",
        "        raise Exception(f\"Scheduler {scheduler} not implemented.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h2nkI9YJPJuH"
      },
      "source": [
        "### Visualizer\n",
        "\n",
        "Function to visualize the results of the training, it prints the confusion matrix and some performance metrics. In addition, it plots all the relevant metrics in the notebook.\n",
        "\n",
        "#### `plot_tsne`\n",
        "\n",
        "This function plots the t-SNE representation of the features extracted from the model. It is used to visualize the features extracted by the model and to check if the model is able to learn the features of the dataset. \n",
        "\n",
        "#### `visualize_results`\n",
        "\n",
        "This function plots the results of the training. It plots \n",
        " - The loss and the accuracy of the training and testing sets\n",
        " - The confusion matrix (on both multiclass and single class)\n",
        " - The t-SNE representation of the features extracted by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2ZQHnBWPJuH"
      },
      "outputs": [],
      "source": [
        "def plot_tsne(\n",
        "        feature_model: nn.Module = None,\n",
        "        classifier_model_s: nn.Module = None,\n",
        "        classifier_model_t: nn.Module = None,\n",
        "        dataset: DataLoader = None,\n",
        "        # all_predictions: np.ndarray = None,\n",
        "        # all_labels: np.ndarray = None,\n",
        "        device: torch.device = DEFAULT_DEVICE,\n",
        "        save_path: str = None,\n",
        "        *args, \n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "    \"\"\"\n",
        "    Plot t-SNE of features.\n",
        "    \"\"\"\n",
        "    feature_model.to(device)\n",
        "    feature_model.eval()\n",
        "    classifier_model_s.to(device)\n",
        "    classifier_model_s.eval()\n",
        "    if classifier_model_t is not None:\n",
        "        classifier_model_t.to(device)\n",
        "        classifier_model_t.eval()\n",
        "        all_predictions_t = []\n",
        "\n",
        "    all_predictions = []\n",
        "    all_predictions_s = []\n",
        "    all_labels = []\n",
        "    n_samples = 0\n",
        "\n",
        "    for batch in dataset:\n",
        "        images, labels = batch\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        features = feature_model(images)\n",
        "        predictions_s = classifier_model_s(features, features_layer = True)\n",
        "        \n",
        "        if classifier_model_t is not None:\n",
        "            predictions_t = classifier_model_t(features, features_layer = True)\n",
        "            all_predictions_t.append(predictions_t.detach().cpu().numpy())\n",
        "\n",
        "        all_predictions.append(features.detach().cpu().numpy())\n",
        "        all_predictions_s.append(predictions_s.detach().cpu().numpy())\n",
        "        \n",
        "        all_labels.append(labels.detach().cpu().numpy())\n",
        "        n_samples += images.shape[0]\n",
        "    \n",
        "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
        "    all_predictions_s = np.concatenate(all_predictions_s, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    if classifier_model_t is not None:\n",
        "        all_predictions_t = np.concatenate(all_predictions_t, axis=0)\n",
        "\n",
        "    # Get tsne\n",
        "    tsne = TSNE(n_components=2, perplexity=DEFAULT_TSNE_PERPLEXITY)\n",
        "    tsne_results = tsne.fit_transform(all_predictions)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=all_labels, cmap='tab10')\n",
        "    plt.colorbar()\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path+\"_tsne_resnet.png\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    # clear plot\n",
        "    plt.clf()\n",
        "    tsne = TSNE(n_components=2, perplexity=DEFAULT_TSNE_PERPLEXITY)\n",
        "    tsne_results_s = tsne.fit_transform(all_predictions_s)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.scatter(tsne_results_s[:, 0], tsne_results_s[:, 1], c=all_labels, cmap='tab10')\n",
        "    plt.colorbar()\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path+\"_tsne_source.png\")\n",
        "    else:\n",
        "        plt.show()\n",
        "    \n",
        "    if classifier_model_t is not None:\n",
        "        plt.clf()\n",
        "        tsne = TSNE(n_components=2, perplexity=DEFAULT_TSNE_PERPLEXITY)\n",
        "        tsne_results_t = tsne.fit_transform(all_predictions_t)\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.scatter(tsne_results_t[:, 0], tsne_results_t[:, 1], c=all_labels, cmap='tab10')\n",
        "        plt.colorbar()\n",
        "        if save_path is not None:\n",
        "            plt.savefig(save_path+\"_tsne_target.png\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "def visualize_results(\n",
        "        feature_model: nn.Module,\n",
        "        classifier_model_s: nn.Module,\n",
        "        eval_dataset: DataLoader,\n",
        "        classifier_model_t: nn.Module = None,\n",
        "        metrics: dict = None,\n",
        "        device: torch.device = DEFAULT_DEVICE,\n",
        "        save_path: str = None,\n",
        "        *args, \n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "    feature_model.to(device)\n",
        "    feature_model.eval()\n",
        "\n",
        "    classifier_model_s.to(device)\n",
        "    classifier_model_s.eval()\n",
        "\n",
        "    if classifier_model_t is not None:\n",
        "        classifier_model_t.to(device)\n",
        "        classifier_model_t.eval()\n",
        "\n",
        "    ### Init variables\n",
        "    all_features = []\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    n_samples = 0\n",
        "    \n",
        "    ### Populate confusion matrix multi-class\n",
        "    for batch in eval_dataset:\n",
        "        images, labels = batch\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        features = feature_model(images)\n",
        "        outputs = classifier_model_s(features)\n",
        "        if classifier_model_t is not None:\n",
        "            outputs += classifier_model_t(features)\n",
        "        _, y = torch.max(outputs, dim=1)\n",
        "        for _f, _label, _pred in zip(features, labels, y):\n",
        "            all_features.append(_f.cpu().detach().numpy())\n",
        "            all_predictions.append(_pred.cpu().detach().numpy().item())\n",
        "            all_labels.append(_label.cpu().detach().numpy().item())\n",
        "        n_samples += images.shape[0]\n",
        "\n",
        "    all_classes = set([DEFAULT_CLASSES[i] for i in all_labels+all_predictions])    \n",
        "    plt.figure(0, figsize=(15, 15))\n",
        "    confusion_m = pd.DataFrame(confusion_matrix(all_labels, all_predictions, normalize='true'),index=DEFAULT_CLASSES,columns=DEFAULT_CLASSES)\n",
        "    sns.heatmap(confusion_m, cmap='icefire', annot = True, xticklabels=True, yticklabels=True, linewidths=0.3)\n",
        "    plt.title('Confusion matrix', fontsize=14)\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path+\"_confusion_matrix.png\")\n",
        "    else:\n",
        "        plt.show()\n",
        "    \n",
        "    ### Populate confusion matrix single-class\n",
        "    additional_infos = {\n",
        "        \"TP\": 0,\n",
        "        \"TN\": 0,\n",
        "        \"FP\": 0,\n",
        "        \"FN\": 0,\n",
        "    }\n",
        "\n",
        "    for y_pred, y_true in zip(all_predictions, all_labels):\n",
        "        if y_pred == y_true:\n",
        "            additional_infos[\"TP\"] += 1\n",
        "            additional_infos[\"TN\"] += DEFAULT_NUM_CLASSES - 1\n",
        "        else:\n",
        "            additional_infos[\"FP\"] += 1\n",
        "            additional_infos[\"FN\"] += DEFAULT_NUM_CLASSES - 1\n",
        "    \n",
        "    print()\n",
        "    print(\n",
        "        pd.DataFrame({\n",
        "            \"Predicted Positive\": additional_infos[\"TP\"],\n",
        "            \"Predicted Negative\": additional_infos[\"FP\"],\n",
        "            \"Actual Positive\": additional_infos[\"FN\"],\n",
        "            \"Actual Negative\": additional_infos[\"TN\"],\n",
        "            }, index=[\"Actual Positive\", \"Actual Negative\"], columns=[\"Predicted Positive\", \"Predicted Negative\"])\n",
        "    )\n",
        "    print()\n",
        "        \n",
        "    ### Print the report for all classes\n",
        "    print(classification_report(all_labels, all_predictions, zero_division=0, target_names=all_classes))\n",
        "    \n",
        "    # Plot the metrics \n",
        "    if metrics is not None:\n",
        "        if \"train/loss_source\" in metrics.keys():\n",
        "            # Plot loss on source and target together\n",
        "            # create a subplot with 1 row and 2 columns\n",
        "            fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "            fig.suptitle('Training')\n",
        "            axs[0].plot(metrics[\"train/loss_source\"], label=\"Source\")\n",
        "            axs[0].plot(metrics[\"train/loss_target\"], label=\"Target\")\n",
        "            axs[0].plot(metrics[\"eval/loss\"], label=\"Test\")\n",
        "            axs[0].set_title('Loss')\n",
        "            axs[0].set_xlabel('Epoch')\n",
        "            axs[0].set_ylabel('Loss')\n",
        "            axs[0].legend()\n",
        "            axs[1].plot(metrics[\"train/accuracy_source\"], label=\"Source Train\")\n",
        "            axs[1].plot(metrics[\"train/accuracy_target\"], label=\"Target Train\")\n",
        "            axs[1].plot(metrics[\"eval/accuracy_source\"], label=\"Source Test\")\n",
        "            axs[1].plot(metrics[\"eval/accuracy_target\"], label=\"Target Test\")\n",
        "            axs[1].plot(metrics[\"eval/accuracy\"], label=\"Total Test\")\n",
        "            axs[1].set_title('Accuracy')\n",
        "            axs[1].set_xlabel('Epoch')\n",
        "            axs[1].set_ylabel('Accuracy')\n",
        "            axs[1].legend()\n",
        "            axs[2].plot(metrics[\"train/discrepancy\"], label=\"Discrepancy\")\n",
        "            axs[2].plot(metrics[\"train/loss_features\"], label=\"Discrepancy generator\")\n",
        "            axs[2].set_title('Discrepancy')\n",
        "            axs[2].set_xlabel('Epoch')\n",
        "            axs[2].set_ylabel('Discrepancy')\n",
        "            axs[2].legend()\n",
        "            if save_path is not None:\n",
        "                plt.savefig(save_path+\"_metrics.png\")\n",
        "            else:\n",
        "                plt.show()\n",
        "\n",
        "        elif \"train/loss\" in metrics.keys():\n",
        "            # Plot loss\n",
        "            fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "            fig.suptitle('Training')\n",
        "            axs[0].plot(metrics[\"train/loss\"], label=\"Loss\")\n",
        "            axs[0].plot(metrics[\"eval/loss\"], label=\"Loss\")\n",
        "            axs[0].set_title('Loss')\n",
        "            axs[0].set_xlabel('Epoch')\n",
        "            axs[0].set_ylabel('Loss')\n",
        "            axs[0].legend()\n",
        "            axs[1].plot(metrics[\"train/accuracy\"], label=\"Accuracy\")\n",
        "            axs[1].plot(metrics[\"eval/accuracy\"], label=\"Accuracy\")\n",
        "            axs[1].set_title('Accuracy')\n",
        "            axs[1].set_xlabel('Epoch')\n",
        "            axs[1].set_ylabel('Accuracy')\n",
        "            axs[1].legend()\n",
        "            if save_path is not None:\n",
        "                plt.savefig(save_path+\"_metrics.png\")\n",
        "            else:\n",
        "                plt.show()\n",
        "            \n",
        "        elif \"eval/loss\" in metrics.keys() and \"train/loss\" not in metrics.keys():\n",
        "            # Plot loss\n",
        "            fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "            fig.suptitle('Test')\n",
        "            axs[0].plot(metrics[\"eval/loss\"], label=\"Loss\")\n",
        "            axs[0].set_title('Loss')\n",
        "            axs[0].set_xlabel('Epoch')\n",
        "            axs[0].set_ylabel('Loss')\n",
        "            axs[0].legend()\n",
        "            axs[1].plot(metrics[\"eval/accuracy\"], label=\"Accuracy\")\n",
        "            axs[1].set_title('Accuracy')\n",
        "            axs[1].set_xlabel('Epoch')\n",
        "            axs[1].set_ylabel('Accuracy')\n",
        "            axs[1].legend()\n",
        "            if save_path is not None:\n",
        "                plt.savefig(save_path+\"_eval.png\")\n",
        "            else:\n",
        "                plt.show()\n",
        "        else:\n",
        "            print(\"No metrics to plot\")\n",
        "        \n",
        "    # Plot t-SNE\n",
        "    plot_tsne(\n",
        "        feature_model=feature_model,\n",
        "        classifier_model_s=classifier_model_s,\n",
        "        classifier_model_t=classifier_model_t,\n",
        "        dataset=eval_dataset,\n",
        "        device=device,\n",
        "        save_path=save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ggozDCTuTwQ"
      },
      "source": [
        "## Maximum Classifier Discrepancy for Unsupervised Domain Adaptation (MCD)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_bFvfdCuTwQ"
      },
      "source": [
        "### Theoretical idea of the paper\n",
        "The idea of the paper is to propose a way to align distributions of the source domain dataset and the target domain dataset by utilizig the task-specific decision boundaries. \n",
        " \n",
        "In the paper presents 3 main protagonists: \n",
        "- 1 generator: used to extract feaures from the inputs\n",
        "- 2 discriminators: used for the classification part, since their role is to classify element given their features.\n",
        "\n",
        "There are 2 goals and with the proposed method we are trying to reach both of them at the same time:\n",
        "1. Maximize the discrepancy between 2 classifiers' outputs to detect target samples that are far from the support of the source.\n",
        "2. Train the feature extractor in order to generate target features near the support to minimize the discrepancy.\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=13hBuS7j2ynA8C7PJ_N3xun0gnjvn08V_\" width=800px>\n",
        "    <figcaption align = \"center\"><b>Theoretical Idea of MCD Method</b></figcaption>\n",
        "</p>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ljqFuoKj3LoC"
      },
      "source": [
        "### Feature extractor\n",
        "\n",
        "In the paper, the authors implemented their own architecture for the generator. However, due to computational complexity problems, we could not use their proposed architectures. Indeed, the training was taking pretty much more than hundreds of epochs. Thus, we came out with the idea of using as Feature extractor a Resnet, removing the last linear layer, in order to output only features. \n",
        "\n",
        "In the class below we implemented a pretrained Resnet, with the possibility of choosing between `Resnet-18`, `Resnet-50` or `Resnet-101` with their equivalent weights. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqG_k7xp3LoC"
      },
      "outputs": [],
      "source": [
        "def get_resnet_model(model: str = 'resnet50', *args, **kwargs) -> nn.Module:\n",
        "    if model == 'resnet50':\n",
        "        return models.resnet50(weights=\"IMAGENET1K_V2\")\n",
        "    elif model == 'resnet101':\n",
        "        return models.resnet101(weights=\"IMAGENET1K_V2\")\n",
        "    elif model == 'resnet18':\n",
        "        return models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "    else:\n",
        "        raise Exception(f\"Model {model} not implemented.\")\n",
        "\n",
        "class FeatureModel(nn.Module):\n",
        "    def __init__(self, resnet_version: str = 'resnet50') -> None:\n",
        "        super(FeatureModel, self).__init__()\n",
        "\n",
        "        #Define the pretrained architecture\n",
        "        resnet = get_resnet_model(resnet_version)\n",
        "\n",
        "        # Save the output size of the last layer because we will need it later\n",
        "        self.output_size = resnet.fc.in_features\n",
        "\n",
        "        # Define the new architecture by removing the last layer\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "            \n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # We have to squeeze the output since it is a 4D tensor and we want a 2D one\n",
        "        x = self.backbone(x).squeeze().squeeze()\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jPOgOAzs3LoD"
      },
      "source": [
        "### Classifier\n",
        "\n",
        "The classifier is a simple 3-layer neural network. The input size is the same as the output size of the feature extractor. The output size is the number of classes in the dataset. For each layer there is a linear layer, a batch normalization layer and a ReLU activation function. Finally, there is a dropout layer to avoid overfitting.\n",
        "\n",
        "Default parameters are:\n",
        " - `input_size`: 512 (Resnet-18)\n",
        " - `hidden_dims`: (256, 128)\n",
        " - `output_size`: 20 (DEFAULT_NUM_CLASSES)\n",
        " - `dropout`: 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HxfYBFr3LoD"
      },
      "outputs": [],
      "source": [
        "class Predictor(nn.Module):\n",
        "    def __init__(self, input_size:int, num_classes:int = DEFAULT_NUM_CLASSES, hidden_dims: tuple = (256, 128), dropout_pr: float = 0.1) -> None:\n",
        "        super(Predictor, self).__init__()\n",
        "        \n",
        "        # Build the model\n",
        "        self.fc1 = nn.Linear(input_size, hidden_dims[0])\n",
        "        self.batch_norm1 = nn.BatchNorm1d(hidden_dims[0])\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
        "        self.batch_norm2 = nn.BatchNorm1d(hidden_dims[1])\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_dims[1], num_classes)\n",
        "        self.batch_norm3 = nn.BatchNorm1d(num_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_pr)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, features_layer: bool = False) -> torch.Tensor:\n",
        "        x = self.fc1(x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        if features_layer:\n",
        "            return x\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WYlpnMLozWgU"
      },
      "source": [
        "## 1) Baseline Approach\n",
        "\n",
        "To define our baseline we decided to use the method described in the paper, adapted for the training supervised only on the source only domain, without any UDA technique. \n",
        "\n",
        "In our case, the UDA part was entering in the game when we were training the 2 classifiers to maximize discrepancy and training the features extractor to minimize the discrepancy between distributions of the models. For this reason we decide to implement our baseline as follows.\n",
        "- Architecture:\n",
        "    - 1 generator for feature extractor: we used a pretrained Resnet-18\n",
        "    - 1 classifier for the classification task\n",
        "- Method:\n",
        "    1. Train in a supervised way on the surce domain, we train the Classifier only\n",
        "    2. Test on the target domain\n",
        "   <!-- , by generating feature from the Features extractor, and computing the classification task through the classifier -->\n",
        "\n",
        "Results are discussed after the run for $P \\rightarrow RW$ and $RW \\rightarrow P$ "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CD8Wwwma3LoD"
      },
      "source": [
        "### Train and Test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Fgv2jXl3LoD"
      },
      "outputs": [],
      "source": [
        "def train_one_step_baseline(\n",
        "        feature_model: nn.Module, \n",
        "        source_model: nn.Module,\n",
        "        optimizer: torch.optim.Optimizer, \n",
        "        criterion: nn.Module, \n",
        "        dataset: DataLoader,\n",
        "        device: str = DEFAULT_DEVICE,\n",
        "    ) -> dict:\n",
        "    # Make sure model is in training mode\n",
        "    source_model.train()\n",
        "    \n",
        "    # Initialize variables\n",
        "    n_samples = 0\n",
        "    cumulative_loss = 0\n",
        "    cumulative_accuracy = 0\n",
        "\n",
        "    # Train loop\n",
        "    for img, label in dataset:\n",
        "        # Move data to device\n",
        "        img, label = img.to(device), label.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        features = feature_model(img)\n",
        "        out = source_model(features)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss : torch.Tensor = criterion(out, label)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #Compute cumulative accuracy and cumulative loss\n",
        "        cumulative_accuracy += compute_accuracy(out, label)\n",
        "        cumulative_loss += loss.item()\n",
        "        n_samples += img.shape[0]\n",
        "\n",
        "    return {\n",
        "        'train/loss': cumulative_loss/n_samples, \n",
        "        'train/accuracy': cumulative_accuracy/n_samples,\n",
        "    }\n",
        "\n",
        "def eval_one_step_baseline(\n",
        "        feature_model: nn.Module, \n",
        "        source_model: nn.Module,\n",
        "        criterion: nn.Module, \n",
        "        dataset: DataLoader,\n",
        "        device: str = DEFAULT_DEVICE,\n",
        "    ) -> dict:\n",
        "\n",
        "    # Make sure model is in evaluation\n",
        "    source_model.eval()\n",
        "    \n",
        "    # Initialize variables\n",
        "    n_samples = 0\n",
        "    cumulative_loss = 0\n",
        "    cumulative_accuracy = 0\n",
        "    # Eval loop\n",
        "    for img, label in dataset:\n",
        "      # Move data to device\n",
        "      img, label = img.to(device), label.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      features = feature_model(img)\n",
        "      out = source_model(features)\n",
        "\n",
        "      # Compute loss\n",
        "      loss : torch.Tensor = criterion(out, label)\n",
        "\n",
        "      #Compute cumulative accuracy and cumulative loss\n",
        "      cumulative_accuracy += compute_accuracy(out, label)\n",
        "      cumulative_loss += loss.item()\n",
        "      n_samples += img.shape[0]\n",
        "\n",
        "    return {\n",
        "        'eval/loss': cumulative_loss/n_samples,\n",
        "        'eval/accuracy': cumulative_accuracy/n_samples,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARAOSEXj3LoD"
      },
      "outputs": [],
      "source": [
        "def train_baseline(\n",
        "        feature_model: nn.Module,\n",
        "        source_model: nn.Module,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        criterion: nn.Module,\n",
        "        train_dataset: DataLoader,\n",
        "        eval_dataset: DataLoader,\n",
        "        scheduler = None,\n",
        "        n_epochs: int = DEFAULT_EPOCHS_BASELINE,\n",
        "        device: str = DEFAULT_DEVICE,\n",
        "        save_name: str = 'baseline.pth',\n",
        "    ) -> float:\n",
        "    # Move models to device\n",
        "    feature_model.to(device)\n",
        "    source_model.to(device)\n",
        "\n",
        "    # Make sure freature model is in evaluation - we don't want to train it here\n",
        "    feature_model.eval()\n",
        "    \n",
        "    # Initialize variables for best model saving\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "\n",
        "    # Initialize variables\n",
        "    start_timer = time.time()\n",
        "\n",
        "    # Store metrics to visualize\n",
        "    metrics_visualize = {\n",
        "        'train/loss': [],\n",
        "        'train/accuracy': [],\n",
        "        'eval/loss': [],\n",
        "        'eval/accuracy': [],\n",
        "    }\n",
        "    \n",
        "    # Train loop\n",
        "    for epoch in range(n_epochs):\n",
        "        # Train\n",
        "        train_results = train_one_step_baseline(feature_model, source_model, optimizer, criterion, train_dataset, device)\n",
        "        # Eval\n",
        "        eval_results = eval_one_step_baseline(feature_model, source_model, criterion, eval_dataset, device)\n",
        "\n",
        "        # Combine results into one dictionary\n",
        "        metrics = {**train_results, **eval_results}\n",
        "\n",
        "        # Log results in wandb\n",
        "        wandb.log(metrics)\n",
        "\n",
        "        # Save best model if test accuracy is better\n",
        "        if eval_results['eval/accuracy'] > best_accuracy:\n",
        "            best_accuracy = eval_results['eval/accuracy']\n",
        "            best_model = copy.deepcopy(source_model)\n",
        "            \n",
        "            # Although it can take some computational time, we save the model after each epoch \n",
        "            # to make sure we don't lose it in case of a crash\n",
        "            torch.save(best_model.state_dict(), os.path.join(LOG_PATH_MODELS, save_name+'.pth'))\n",
        "\n",
        "        # Print results to console\n",
        "        make_log_print(\"Train\", (epoch+1, n_epochs), time.time()-start_timer, metrics)\n",
        "\n",
        "        # Save metrics to visualize\n",
        "        for key, value in metrics.items():\n",
        "            metrics_visualize[key].append(value)\n",
        "\n",
        "        # Step scheduler \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "    \n",
        "    # Visualize results like loss, accuracy, confusion matrix and t-sne\n",
        "    visualize_results(\n",
        "        feature_model=feature_model,\n",
        "        classifier_model_s=best_model,\n",
        "        eval_dataset=eval_dataset,\n",
        "        metrics=metrics_visualize, \n",
        "        device=device, \n",
        "        save_path=os.path.join(LOG_PATH_IMAGES, save_name))\n",
        "\n",
        "    # Return best accuracy since we want to compare it with other models\n",
        "    # No neeed to return best model since we save it \n",
        "    return best_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTamkA-PyN1g"
      },
      "source": [
        "### Runs\n",
        "\n",
        "We will evaluate the baseline in 2 directions w.r.t. the dataset:\n",
        "- $P → RW$ : training on the product domain and testing the model on the real woRWd domain\n",
        "- $RW → P$ : training on the real woRWd domain and testing the model on the product domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp7xsPgqzCu-"
      },
      "source": [
        "### $P \\rightarrow RW$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "be32e6696f184b88980f5b9b0211b949",
            "6737094c2c284cc18b239e240bac065a",
            "f8e67a17714240409d0406779f75b4fc",
            "812d66b3989648bab09a13768f20dfde",
            "1e9a6a9e99f94ed5bf3e2eff7dfde74b",
            "bffc5f8f29e54feb833fdf6a1f4d1560",
            "892e357dd136460f9c627b523dba9e91",
            "fd5556e9746941e9b4461e0bfa36d38f"
          ]
        },
        "id": "kLsQyswN3LoE",
        "outputId": "9cfa90d4-c97a-443c-f344-500fa05eb5fd"
      },
      "outputs": [],
      "source": [
        "# Variable definition\n",
        "feature_model = FeatureModel(resnet_version='resnet18')\n",
        "source_model = Predictor(input_size=feature_model.output_size)\n",
        "optimizer = get_optimizer(source_model, optimizer='sgd', lr=DEFAULT_LEARNING_RATE)\n",
        "criterion = get_criterion(criterion='cross_entropy')\n",
        "\n",
        "# Initialize wandb\n",
        "experiment_name = 'Baseline_P_RW'\n",
        "\n",
        "wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=DEFAULT_WANDB_PROJECT_NAME, \n",
        "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    name=experiment_name, \n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        \"feature_model\": feature_model.__class__.__name__,\n",
        "        \"source_model\": source_model.__class__.__name__,\n",
        "        \"source_optimizer\": optimizer.__class__.__name__,\n",
        "        \"criterion\": criterion.__class__.__name__,\n",
        "        \"training_dataset\": \"Products\",\n",
        "        \"evaluation_dataset\": \"RealWorld\",\n",
        "        \"training_type\": \"Baseline P-RW\",\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
        "        \"epochs\": DEFAULT_EPOCHS_BASELINE,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Train\n",
        "accuracy_baseline_p_rw = train_baseline(\n",
        "    feature_model=feature_model,\n",
        "    source_model=source_model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    train_dataset=train_loader_products,\n",
        "    eval_dataset=test_loader_reals,\n",
        "    save_name=experiment_name\n",
        "    )\n",
        "\n",
        "# Close wandb\n",
        "wandb.finish()\n",
        "\n",
        "# Delete variables to free memory\n",
        "del feature_model, source_model, optimizer, criterion, experiment_name\n",
        "\n",
        "# Clear memory\n",
        "if DEFAULT_DEVICE.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Print results\n",
        "print('Accuracy baseline P-RW: {:.2f}'.format(accuracy_baseline_p_rw*100))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mdEbpemN0BO2"
      },
      "source": [
        "#### Results\n",
        "Here we report the results of the baseline on the $P \\rightarrow RW$ direction.\n",
        "\n",
        "##### Final results\n",
        "\n",
        "<center>\n",
        "\n",
        "| | Accuracy | Loss | \n",
        "| --- | :---: | --- |\n",
        "| Train | 88.9 % | 0.002 | \n",
        "| Test | 80.7 % | 0.004 |\n",
        "\n",
        "</center>\n",
        "\n",
        "##### Metrics\n",
        "\n",
        "<center>\n",
        "\n",
        "| | precision | recall | f1-score | support |\n",
        "| --- | :---: | :---: | :---: | :---: |\n",
        "| backpack | 0.96 | 0.96 | 0.96 | 24 |\n",
        "| bookcase | 0.87 | 0.87 | 0.87 | 15 |\n",
        "| car jack | 1.00 | 0.86 | 0.92 | 21 |\n",
        "| comb | 0.91 | 0.95 | 0.93 | 21 |\n",
        "| crown | 1.00 | 0.71 | 0.83 | 17 |\n",
        "| file cabinet | 0.83 | 0.71 | 0.77 | 21 |\n",
        "| flat iron | 1.00 | 0.73 | 0.84 | 22 |\n",
        "| game controller | 0.64 | 0.95 | 0.76 | 22 |\n",
        "| glasses | 0.82 | 0.53 | 0.64 | 17 |\n",
        "| helicopter | 1.00 | 0.89 | 0.94 | 19 |\n",
        "| ice skates | 0.93 | 0.70 | 0.80 | 20 |\n",
        "| letter tray | 0.57 | 1.00 | 0.72 | 17 |\n",
        "| monitor | 0.81 | 0.63 | 0.71 | 27 |\n",
        "| mug | 0.89 | 0.94 | 0.91 | 17 |\n",
        "| network switch | 0.68 | 1.00 | 0.81 | 19 |\n",
        "| over-ear headphones | 0.52 | 0.94 | 0.67 | 16 |\n",
        "| pen | 0.77 | 0.85 | 0.81 | 20 |\n",
        "| purse | 0.94 | 0.71 | 0.81 | 21 |\n",
        "| stand mixer | 0.78 | 0.61 | 0.68 | 23 |\n",
        "| stroller | 0.88 | 0.71 | 0.79 | 21 |\n",
        "| | | | | |\n",
        "| accuracy | | | 0.81 | 400 |\n",
        "| macro avg | 0.84 | 0.81 | 0.81 | 400 |\n",
        "| weighted avg | 0.84 | 0.81 | 0.81 | 400 |\n",
        "\n",
        "</center>\n",
        "\n",
        "##### Loss and Accuracy curves\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=17MQhezyHGicYNNiS_D43FRtAPVPrddVK\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1k7IXzX4gTDGvcZc6LwOwlvcICVSa9KLr\" width=400px>\n",
        "    <figcaption align = \"center\"><b>Loss & Accuracy</b></figcaption>\n",
        "</p>\n",
        "\n",
        "##### Confusion Matrix\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1ccL_83f5IGKVjBTQfZ_FWy69c7eryHQK\" width=600px>\n",
        "    <figcaption align = \"center\"><b>Confusion Matrix</b></figcaption>\n",
        "</p>\n",
        "\n",
        "##### t-SNE\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1MePvXiItwbH8fEM7VrL0hm8IZdHoN_Yf\" width=400px>\n",
        "    <figcaption align = \"center\"><b>t-SNE</b></figcaption>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg-3lMKt0J-J"
      },
      "source": [
        "### $RW \\rightarrow P$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "019f5cab445e450aae3898e9531bb65b",
            "c7be1f2d67d5402ebef40a0411d73cdb",
            "cc99f98bea624e1cb9d9a0c4762587e6",
            "624f6cd693a64210a6a5db96a7fbc91d",
            "d4bda0833c334b509717c01f68260bcf",
            "c20caf69e0984953aa64a2640ab7f033",
            "aad5e9bbbc464e1387e2ab20c6109011",
            "b5ad309825c846daa01837eed020277c"
          ]
        },
        "id": "xFZauvvM3LoE",
        "outputId": "13b3d5d6-346d-42ff-b65d-60cd703fedf6"
      },
      "outputs": [],
      "source": [
        "# Variable definition\n",
        "feature_model = FeatureModel(resnet_version='resnet18')\n",
        "source_model = Predictor(input_size=feature_model.output_size)\n",
        "optimizer = get_optimizer(source_model, optimizer='sgd', lr=DEFAULT_LEARNING_RATE)\n",
        "criterion = get_criterion(criterion='cross_entropy')\n",
        "\n",
        "# Initialize wandb\n",
        "experiment_name = 'Baseline_RW_P'\n",
        "\n",
        "wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=DEFAULT_WANDB_PROJECT_NAME, \n",
        "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    name=experiment_name, \n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        \"feature_model\": feature_model.__class__.__name__,\n",
        "        \"source_model\": source_model.__class__.__name__,\n",
        "        \"source_optimizer\": optimizer.__class__.__name__,\n",
        "        \"criterion\": criterion.__class__.__name__,\n",
        "        \"training_dataset\": \"RealWorld\",\n",
        "        \"evaluation_dataset\": \"Products\",\n",
        "        \"training_type\": \"Baseline RW-P\",\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
        "        \"epochs\": DEFAULT_EPOCHS_BASELINE,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Train\n",
        "accuracy_baseline_rw_p = train_baseline(\n",
        "    feature_model=feature_model,\n",
        "    source_model=source_model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    train_dataset=train_loader_reals,\n",
        "    eval_dataset=test_loader_products,\n",
        "    save_name=experiment_name\n",
        "    )\n",
        "\n",
        "# Close wandb\n",
        "wandb.finish()\n",
        "\n",
        "# Delete variables to free memory\n",
        "del feature_model, source_model, optimizer, criterion, experiment_name\n",
        "\n",
        "# Empty cache\n",
        "if DEFAULT_DEVICE.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Print results\n",
        "print('Accuracy baseline RW-P: {:.2f}'.format(accuracy_baseline_rw_p*100))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ULUaV68DPJuL"
      },
      "source": [
        "#### Results\n",
        "\n",
        "Here we report the results of the baseline on the $RW \\rightarrow P$ direction.\n",
        "\n",
        "##### Final results\n",
        "\n",
        "<center>\n",
        "\n",
        "| | Accuracy | Loss |\n",
        "| --- | :---: | --- |\n",
        "| Train | 89.3 % | 0.002 |\n",
        "| Test | 92.2 % | 0.002 |\n",
        "\n",
        "</center>\n",
        "\n",
        "##### Metrics\n",
        "\n",
        "<center>\n",
        "\n",
        "| | precision | recall | f1-score | support |\n",
        "| --- | :---: | :---: | :---: | :---: |\n",
        "| backpack | 0.87 | 0.72 | 0.79 | 18 |\n",
        "| bookcase | 0.94 | 1.00 | 0.97 | 16 |\n",
        "| car jack | 1.00 | 0.95 | 0.97 | 19 |\n",
        "| comb | 0.95 | 1.00 | 0.97 | 19 |\n",
        "| crown | 0.86 | 0.95 | 0.90 | 19 |\n",
        "| file cabinet | 0.92 | 0.96 | 0.94 | 24 |\n",
        "| flat iron | 1.00 | 0.71 | 0.83 | 17 |\n",
        "| game controller | 0.89 | 1.00 | 0.94 | 17 |\n",
        "| glasses | 0.88 | 1.00 | 0.94 | 15 |\n",
        "| helicopter | 0.93 | 0.93 | 0.93 | 29 |\n",
        "| ice skates | 0.89 | 1.00 | 0.94 | 17 |\n",
        "| letter tray | 0.86 | 0.95 | 0.90 | 19 |\n",
        "| monitor | 1.00 | 0.95 | 0.97 | 20 |\n",
        "| mug | 0.83 | 0.90 | 0.86 | 21 |\n",
        "| network switch | 0.92 | 0.79 | 0.85 | 29 |\n",
        "| over-ear headphones | 0.94 | 0.81 | 0.87 | 21 |\n",
        "| pen | 0.83 | 1.00 | 0.91 | 20 |\n",
        "| purse | 1.00 | 0.94 | 0.97 | 16 |\n",
        "| stroller | 1.00 | 1.00 | 1.00 | 24 |\n",
        "| stand mixer | 1.00 | 0.95 | 0.97 | 20 |\n",
        "| | | | | |\n",
        "| accuracy | | | 0.92 | 400 |\n",
        "| macro avg | 0.93 | 0.93 | 0.92 | 400 |\n",
        "| weighted avg | 0.93 | 0.92 | 0.92 | 400 |\n",
        "\n",
        "</center>\n",
        "\n",
        "##### Loss and Accuracy curves\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1KXR0IBuo8TqPP3qtLWYz1TL4imSv_8M4\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1WGlQdMffQfDs1cDY4NkPwecemyz_dKQW\" width=400px>\n",
        "    <figcaption align = \"center\"><b>Loss & Accuracy</b></figcaption>\n",
        "</p>\n",
        "\n",
        "##### Confusion Matrix\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1AjQiAWXVMXEhjg97MWfC_rhceUeBrqrO\" width=600px>\n",
        "    <figcaption align = \"center\"><b>Confusion Matrix</b></figcaption>\n",
        "</p>\n",
        "\n",
        "##### t-SNE\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1myFwRjs6_xVE1op-KSdD34DGbXEInupD\" width=400px>\n",
        "    <figcaption align = \"center\"><b>t-SNE</b></figcaption>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discussion\n",
        "\n",
        "Thanks to the pretrained Resnet-18 as features extractor, both directions trainings start with a high accuracy. Moreover, they reach very good results for the baselines in just few epochs. <!-- We note that the features extractor model is not trained in the baseline, so the only model trained is the classifier, which has to learn how classify at its best the features coming from the Resnet.  -->\n",
        "\n",
        "As we can see from the tables the accuarcy of $P \\rightarrow RW$ is much lower than $RW \\rightarrow P$. We think that this fact is due to the difference of training domains. In fact, in the first case the model has to learn from domain poor of details, and it is evaluated on a much more complex domain. On the other hand, in the second case the model can learn very well from images which are more complex: the features are surely more characterizing the object in the image with respect to the corresponding images in the other domain.\n",
        "\n",
        "The confusion matrix is a demonstration of this statement: the matrix for $P \\rightarrow RW$ is less sparse than the one for $RW \\rightarrow P$, which means that the model cannot understand very well where it has to classify the image in input.\n",
        "Also the t-SNE representation for $P \\rightarrow RW$ is more confused, and the different classes are not very grouped together as for the versus of $RW \\rightarrow P$.\n",
        "\n",
        "We report under this discussion some results for both cases. \n",
        "\n",
        "#### Classifiers\n",
        "\n",
        "We want to underline the fact that the classifiers can already learn very well in the baseline, and this fact can be seen by the difference from the t-SNE representation before the classifier and at the end of it for training. The classes are already very well separated, even though there are still some outliers, especially in the $P \\rightarrow RW$ direction, which, as we wrote before, is the most difficult direction to learn. Indeed, the t-SNE representation of the Resnet in that case is pretty confused, while the second case the classes are already well separated before the classifier.\n",
        "\n",
        "We report both t-SNE representations of both baseline before and after related classifiers.\n",
        "\n",
        "<p align=\"center\">\n",
        "    <p align=\"center\">\n",
        "        <img src=\"https://drive.google.com/uc?id=18ecf6MmDEBs6LfKvDreGn55-IDa9fKym\" width=400px>\n",
        "        <img src=\"https://drive.google.com/uc?id=1MePvXiItwbH8fEM7VrL0hm8IZdHoN_Yf\" width=400px>\n",
        "        <figcaption align = \"center\"><b>P -> RW: t-SNE before and after classifier</b></figcaption>\n",
        "    </p>\n",
        "    <p align=\"center\">\n",
        "        <img src=\"https://drive.google.com/uc?id=1589q3xPdj5ykOD6Kd41-IVEt-C2RFFdm\" width=400px>\n",
        "        <img src=\"https://drive.google.com/uc?id=1myFwRjs6_xVE1op-KSdD34DGbXEInupD\" width=400px>\n",
        "        <figcaption align = \"center\"><b>RW -> P : t-SNE before and after classifier</b></figcaption>\n",
        "    </p>\n",
        "</p>    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm-VUlVj3LoE"
      },
      "source": [
        "## 2) Upperbound\n",
        "We now compute the upperbound for both directions and each upperbound we will name it as:\n",
        "- $P$ : $acc_{up}^P$\n",
        "- $RW$ : $acc_{up}^{RW}$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zB3CiaVz3LoF"
      },
      "source": [
        "### Upperbound $P$\n",
        "\n",
        "> MEMO : train on $RW_{train}$ and test on $RW_{test}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GhxwcbhPBYZ"
      },
      "outputs": [],
      "source": [
        "# Variable definition\n",
        "feature_model = FeatureModel(resnet_version='resnet18')\n",
        "source_model = Predictor(input_size=feature_model.output_size)\n",
        "optimizer = get_optimizer(source_model, optimizer='sgd', lr=DEFAULT_LEARNING_RATE)\n",
        "criterion = get_criterion(criterion='cross_entropy')\n",
        "\n",
        "# Initialize wandb\n",
        "experiment_name = 'Upperbound_RW'\n",
        "\n",
        "wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=DEFAULT_WANDB_PROJECT_NAME, \n",
        "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    name=experiment_name, \n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        \"feature_model\": feature_model.__class__.__name__,\n",
        "        \"source_model\": source_model.__class__.__name__,\n",
        "        \"source_optimizer\": optimizer.__class__.__name__,\n",
        "        \"criterion\": criterion.__class__.__name__,\n",
        "        \"training_dataset\": \"RealWorld\",\n",
        "        \"evaluation_dataset\": \"RealWorld\",\n",
        "        \"training_type\": \"Upperbound RW\",\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
        "        \"epochs\": DEFAULT_EPOCHS_BASELINE,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Train\n",
        "accuracy_upperbound_rw = train_baseline(\n",
        "    feature_model=feature_model,\n",
        "    source_model=source_model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    train_dataset=train_loader_reals,\n",
        "    eval_dataset=test_loader_reals,\n",
        "    save_name=experiment_name\n",
        ")\n",
        "\n",
        "# Close wandb\n",
        "wandb.finish()\n",
        "\n",
        "# Delete variables to free memory\n",
        "del feature_model, source_model, optimizer, criterion, experiment_name\n",
        "\n",
        "# Empty cache\n",
        "if DEFAULT_DEVICE.type == 'cuda':\n",
        "    torch.cuda.empty_cache() \n",
        "\n",
        "# Print results\n",
        "print('Accuracy upperbound RW: {:.2f}'.format(accuracy_upperbound_rw*100))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q_hF77k2PJuM"
      },
      "source": [
        "### Results\n",
        "\n",
        "Here we report the results of the baseline on the $P \\rightarrow RW$ direction.\n",
        "\n",
        "#### Final results\n",
        "\n",
        "<center>\n",
        "\n",
        "| | Accuracy | Loss |\n",
        "| --- | :---: | --- |\n",
        "| Train | 91.2 % | 0.001 |\n",
        "| Test | 90.5 % | 0.002 |\n",
        "\n",
        "</center>\n",
        "\n",
        "#### Metrics\n",
        "\n",
        "<!-- <center>\n",
        "\n",
        "| | precision | recall | f1-score | support |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| letter tray | 0.68 | 1.00 | 0.81 | 17 |\n",
        "| over-ear headphones | 0.74 | 0.85 | 0.79 | 20 |\n",
        "| purse | 0.94 | 1.00 | 0.97 | 15 |\n",
        "| pen | 0.91 | 0.91 | 0.91 | 22 |\n",
        "| mug | 1.00 | 0.95 | 0.98 | 21 |\n",
        "| ice skates | 0.94 | 0.73 | 0.82 | 22 |\n",
        "| bookcase | 0.94 | 0.94 | 0.94 | 16 |\n",
        "| crown | 1.00 | 0.95 | 0.98 | 21 |\n",
        "| network switch | 0.86 | 1.00 | 0.93 | 19 |\n",
        "| monitor | 1.00 | 1.00 | 1.00 | 19 |\n",
        "| glasses | 0.89 | 0.76 | 0.82 | 21 |\n",
        "| file cabinet | 0.79 | 0.70 | 0.75 | 27 |\n",
        "| helicopter | 0.95 | 0.95 | 0.95 | 20 |\n",
        "| flat iron | 0.89 | 1.00 | 0.94 | 24 |\n",
        "| stroller | 0.84 | 0.94 | 0.89 | 17 |\n",
        "| game controller | 0.94 | 0.94 | 0.94 | 17 |\n",
        "| backpack | 0.89 | 1.00 | 0.94 | 17 |\n",
        "| stand mixer | 0.82 | 0.61 | 0.70 | 23 |\n",
        "| comb | 1.00 | 0.95 | 0.98 | 21 |\n",
        "| car jack | 0.95 | 0.86 | 0.90 | 21 |\n",
        "| | | | | |\n",
        "| accuracy | | | 0.89 | 400 |\n",
        "| macro avg | 0.90 | 0.90 | 0.90 | 400 |\n",
        "| weighted avg | 0.90 | 0.89 | 0.89 | 400 |\n",
        "\n",
        "</center> -->\n",
        "\n",
        "<center>\n",
        "\n",
        "| | precision | recall | f1-score | support |\n",
        "| --- | :---: | :---: | :---: | :---: |\n",
        "| backpack | 0.89 | 1.00 | 0.94 | 17 |\n",
        "| bookcase | 0.94 | 0.94 | 0.94 | 16 |\n",
        "| car jack | 0.95 | 0.86 | 0.90 | 21 |\n",
        "| comb | 1.00 | 0.95 | 0.98 | 21 |\n",
        "| crown | 1.00 | 0.95 | 0.98 | 21 |\n",
        "| file cabinet | 0.79 | 0.70 | 0.75 | 27 |\n",
        "| flat iron | 0.89 | 1.00 | 0.94 | 24 |\n",
        "| game controller | 0.94 | 0.94 | 0.94 | 17 |\n",
        "| glasses | 0.89 | 0.76 | 0.82 | 21 |\n",
        "| helicopter | 0.95 | 0.95 | 0.95 | 20 |\n",
        "| ice skates | 0.94 | 0.73 | 0.82 | 22 |\n",
        "| letter tray | 0.68 | 1.00 | 0.81 | 17 |\n",
        "| monitor | 1.00 | 1.00 | 1.00 | 19 |\n",
        "| mug | 1.00 | 0.95 | 0.98 | 21 |\n",
        "| network switch | 0.86 | 1.00 | 0.93 | 19 |\n",
        "| over-ear headphones | 0.74 | 0.85 | 0.79 | 20 |\n",
        "| pen | 0.91 | 0.91 | 0.91 | 22 |\n",
        "| purse | 0.94 | 1.00 | 0.97 | 15 |\n",
        "| stand mixer | 0.82 | 0.61 | 0.70 | 23 |\n",
        "| stroller | 0.84 | 0.94 | 0.89 | 17 |\n",
        "| | | | | |\n",
        "| accuracy | | | 0.89 | 400 |\n",
        "| macro avg | 0.90 | 0.90 | 0.90 | 400 |\n",
        "| weighted avg | 0.90 | 0.89 | 0.89 | 400 |\n",
        "\n",
        "</center>\n",
        "\n",
        "#### Loss and Accuracy curves\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1s6ye1ogF8ZUR5Q6roVvVECNn22IzF4r6\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1WzNFNOEDn9GOLkJ3525p4WeEbKhwMAOD\" width=400px>\n",
        "    <figcaption align = \"center\"><b>Loss & Accuracy</b></figcaption>\n",
        "</p>\n",
        "\n",
        "#### Confusion Matrix\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1libsX7VuId7voTl_xpEqheP44NJVoTzS\" width=600px>\n",
        "    <figcaption align = \"center\"><b>Confusion Matrix</b></figcaption>\n",
        "</p>\n",
        "\n",
        "#### t-SNE\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=133bxFUsiDYEI2A0XWWMYRC9DKPmR2W4p\" width=400px>\n",
        "    <figcaption align = \"center\"><b>t-SNE</b></figcaption>\n",
        "</p>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TWgzNFf93LoF"
      },
      "source": [
        "### Upperbound $RW$\n",
        "\n",
        "> MEMO : train on $P_{train}$ and test on $P_{test}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQKwlxfPPJuM"
      },
      "outputs": [],
      "source": [
        "# Variable definition\n",
        "feature_model = FeatureModel(resnet_version='resnet18')\n",
        "source_model = Predictor(input_size=feature_model.output_size)\n",
        "optimizer = get_optimizer(source_model, optimizer='sgd', lr=DEFAULT_LEARNING_RATE)\n",
        "criterion = get_criterion(criterion='cross_entropy')\n",
        "\n",
        "# Initialize wandb\n",
        "experiment_name = 'Upperbound_P'\n",
        "\n",
        "wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=DEFAULT_WANDB_PROJECT_NAME, \n",
        "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    name=experiment_name, \n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        \"feature_model\": feature_model.__class__.__name__,\n",
        "        \"source_model\": source_model.__class__.__name__,\n",
        "        \"source_optimizer\": optimizer.__class__.__name__,\n",
        "        \"criterion\": criterion.__class__.__name__,\n",
        "        \"training_dataset\": \"Products\",\n",
        "        \"evaluation_dataset\": \"Products\",\n",
        "        \"training_type\": \"Upperbound P\",\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
        "        \"epochs\": DEFAULT_EPOCHS_BASELINE,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Train\n",
        "accuracy_upperbound_p = train_baseline(\n",
        "    feature_model=feature_model,\n",
        "    source_model=source_model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    train_dataset=train_loader_products,\n",
        "    eval_dataset=test_loader_products,\n",
        "    save_name=experiment_name\n",
        ")\n",
        "\n",
        "# Close wandb\n",
        "wandb.finish()\n",
        "\n",
        "# Delete variables to free memory\n",
        "del feature_model, source_model, optimizer, criterion, experiment_name\n",
        "\n",
        "# Clear memory\n",
        "if DEFAULT_DEVICE.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Print results\n",
        "print('Accuracy upperbound P: {:.2f}'.format(accuracy_upperbound_p*100))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\n",
        "\n",
        "Here we report the results of the upperbound on the $RW \\rightarrow P$ direction.\n",
        "\n",
        "#### Final results\n",
        "\n",
        "<center>\n",
        "\n",
        "| | Accuracy | Loss |\n",
        "| --- | :---: | --- |\n",
        "| Train | 88.9 % | 0.002 |\n",
        "| Test | 97.3 % | 0.001 |\n",
        "\n",
        "</center>\n",
        "\n",
        "#### Metrics\n",
        "\n",
        "<center>\n",
        "\n",
        "| | precision | recall | f1-score | support |\n",
        "| --- | :---: | :---: | :---: | :---: |\n",
        "| backpack | 1.00 | 1.00 | 1.00 | 17 |\n",
        "| bookcase | 1.00 | 1.00 | 1.00 | 20 |\n",
        "| car jack | 0.94 | 1.00 | 0.97 | 29 |\n",
        "| comb | 1.00 | 1.00 | 1.00 | 20 |\n",
        "| crown | 1.00 | 1.00 | 1.00 | 19 |\n",
        "| file cabinet | 1.00 | 0.95 | 0.97 | 20 |\n",
        "| flat iron | 0.82 | 0.78 | 0.80 | 18 |\n",
        "| game controller | 1.00 | 0.90 | 0.95 | 21 |\n",
        "| glasses | 1.00 | 1.00 | 1.00 | 24 |\n",
        "| helicopter | 1.00 | 1.00 | 1.00 | 24 |\n",
        "| ice skates | 0.82 | 0.86 | 0.84 | 21 |\n",
        "| letter tray | 1.00 | 0.89 | 0.94 | 19 |\n",
        "| monitor | 0.84 | 1.00 | 0.91 | 16 |\n",
        "| mug | 1.00 | 1.00 | 1.00 | 19 |\n",
        "| network switch | 0.94 | 1.00 | 0.97 | 17 |\n",
        "| over-ear headphones | 1.00 | 1.00 | 1.00 | 15 |\n",
        "| pen | 1.00 | 1.00 | 1.00 | 17 |\n",
        "| purse | 1.00 | 1.00 | 1.00 | 19 |\n",
        "| stand mixer | 0.94 | 1.00 | 0.97 | 16 |\n",
        "| stroller | 0.96 | 0.90 | 0.93 | 29 |\n",
        "| | | | | |\n",
        "| accuracy | | | 0.96 | 400 |\n",
        "| macro avg | 0.96 | 0.96 | 0.96 | 400 |\n",
        "| weighted avg | 0.96 | 0.96 | 0.96 | 400 |\n",
        "\n",
        "</center>\n",
        "\n",
        "#### Loss and Accuracy curves\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1eEYSm9sFjaUOqy_6jg-wbc9xU2zOpqxD\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1DdF90y6hqwAH3gClgqmnpmdhnU8oPanF\" width=400px>\n",
        "    <figcaption align = \"center\"><b>Loss & Accuracy</b></figcaption>\n",
        "</p>\n",
        "\n",
        "#### Confusion Matrix\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1CbFhOHQPUYbLHium1gu8Fvlk1P9mzA5g\" width=600px>\n",
        "    <figcaption align = \"center\"><b>Confusion Matrix</b></figcaption>\n",
        "</p>\n",
        "\n",
        "#### t-SNE\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1jZlEJe906sZ0fyfsmUcjP0zki6KFD4Ek\" width=400px>\n",
        "    <figcaption align = \"center\"><b>t-SNE</b></figcaption>\n",
        "</p>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\n",
        "\n",
        "As in the baseline, only the classifier is trained. In the baseline, the classifier is trained on the source domain and tested on the target domain. In the upperbound the classifier is trained on the target domain and tested on the source domain.\n",
        "\n",
        "The upperbounds are very important: they give an ideal maximum value for the performance. In this task there is the $acc_{up}^P$ that tells how much is the accuracy for $RW \\rightarrow P$, while $acc_{up}^RW$ for $P \\rightarrow RW$.\n",
        "\n",
        "In the next sections we report some metrics that we will use for the next cells.\n",
        "\n",
        "### Results baseline and upperbounds\n",
        "\n",
        "From the baseline and the upperbound we summarize the results (Accuracies) in the table below.\n",
        "\n",
        "<center>\n",
        "\n",
        "| | Baseline | Upperbound |\n",
        "| :---: | :---: | :---: |\n",
        "| $P \\rightarrow RW$ | 80.7 % | 90.5 % |\n",
        "| $RW \\rightarrow P$  | 92.2 % | 97.3 % |\n",
        "\n",
        "</center>\n",
        "\n",
        "### Gain\n",
        "\n",
        "Table below shows the maximum possible gain we can achieve:\n",
        "\n",
        "<center>\n",
        "\n",
        "| | Gain |\n",
        "| :---: | :---: |\n",
        "| $P \\rightarrow RW$ | 9.8 % |\n",
        "| $RW \\rightarrow P$ | 5.1 % |"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yY2bnYcDzcF9"
      },
      "source": [
        "## 3) Advanced : test the UDA component\n",
        "### Maximum Classifier Discrepancy Approach\n",
        "\n",
        "MCD is a novel adversarial training method that tries to minimize the discrepancy between the source and the target domain. The method is based on the following idea: if the source and the target domain are similar, then the classifier trained on the source domain should be able to classify the target domain with a high accuracy.\n",
        "\n",
        "After the baseline and the upperbound results, we can now try the UDA component and the method proposed in the paper. \n",
        "\n",
        "For this part it is important to apply the following steps training simultaneously:\n",
        "- **Step 1** : train in a superised way on the source domain \n",
        "- **Step 2** : train in an unsupervised way on the target domain\n",
        "\n",
        "In the paper the previous steps are obviously readjusted for the architectures:\n",
        "- **Step A** : train in a superised way on the source domain the Feature Extractor and both Classifiers\n",
        "- **Step B** : train in an unsupervised way on the target domain\n",
        "    - Step B.1 : Fix the Generator and train in an unsupervised way the 2 Classifiers on the target domain\n",
        "    - Step B.2 : Fix the Classifiers and train in an unsupervised way the Generator on the target domain\n",
        "\n",
        "Step A is crucial because it helps to obtain task-specific discriminative features, Step B.1 is useful to train the classifiers as a discriminator for a fixed generator while Step B.2 is needed to train the generator in order to minimize the discrepancy for the fixed classifiers.\n",
        "\n",
        "When testing the obtained model, the actual prediction is given by the combination of the source classifier's prediction and the target classifier's prediction. \n",
        "\n",
        "In the following sections there is the code implementation and the results for each versus."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_K1IjBM13LoB"
      },
      "source": [
        "### Discrepancy Loss\n",
        "\n",
        "The discrepancy loss is defined as the difference between the source and the target domain. In the second and third step the discrepancy loss must be maximized and minimized respectively, in order to have the model learning in an adversarial manner.\n",
        "\n",
        "$$\\mathcal{D}_{loss}(p_1, p_2) = \\frac{1}{K} \\sum_{k=1}^K |p_{1_{k}} - p_{2_{k}}| $$\n",
        "\n",
        "where the $p_{1_{k}}$ and $p_{2_{k}}$ denote probability output of $p_1$ and $p_2$ for class $k$ respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlGls5n33LoB"
      },
      "outputs": [],
      "source": [
        "# Discrepancy function\n",
        "def discrepancy(out1: torch.Tensor, out2: torch.Tensor) -> torch.Tensor:\n",
        "    d = 0\n",
        "    samples = 0\n",
        "    for a, b in zip(out1, out2):\n",
        "        for i in range(len(a)):\n",
        "            d += torch.abs(a[i] - b[i])\n",
        "            samples += 1\n",
        "    \n",
        "    return d / samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogGGHd5s3LoG"
      },
      "source": [
        "### Train and Test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud5acr_zjmzf"
      },
      "outputs": [],
      "source": [
        "def train_mcd_one_step(\n",
        "        features_model: nn.Module, \n",
        "        classifiers: Tuple[nn.Module, nn.Module],\n",
        "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.Optimizer, torch.optim.Optimizer], \n",
        "        criterion: nn.Module, \n",
        "        datasets: Tuple[DataLoader, DataLoader],\n",
        "        discrepancy_fn: Callable,\n",
        "        k: int = DEFAULT_K_STEPS,\n",
        "        device: str = DEFAULT_DEVICE,\n",
        "    ) -> dict:\n",
        "    ### For NLLLoss\n",
        "    # m = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    # Extract classifiers\n",
        "    classifier_source, classifier_target = classifiers\n",
        "\n",
        "    # Extract optimizers\n",
        "    optimizer_features, optimizer_source, optimizer_target = optimizers\n",
        "\n",
        "    # Extract datasets\n",
        "    train_loader_source, train_loader_target = datasets\n",
        "\n",
        "    # Set models to train mode\n",
        "    features_model.train()\n",
        "    classifier_source.train()\n",
        "    classifier_target.train()\n",
        "    \n",
        "    # Initialize cumulative variables\n",
        "    cum_loss_source = 0\n",
        "    cum_loss_target = 0\n",
        "    cum_accuracy_source = 0\n",
        "    cum_accuracy_target = 0\n",
        "    n_samples = 0\n",
        "\n",
        "    # Train loop    \n",
        "    for (x_source, y_source), (x_target, y_target) in zip(train_loader_source, train_loader_target):\n",
        "        # Move to device\n",
        "        x_source, y_source = x_source.to(device), y_source.to(device)\n",
        "        x_target, y_target = x_target.to(device), y_target.to(device)\n",
        "\n",
        "        # Extract features and compute output\n",
        "        features_source = features_model(x_source)\n",
        "        out_source = classifier_source(features_source)\n",
        "        out_target = classifier_target(features_source)\n",
        "\n",
        "        # Compute loss\n",
        "        loss_source = criterion(out_source, y_source)\n",
        "        loss_target = criterion(out_target, y_source)\n",
        "        loss_s: torch.Tensor = loss_source + loss_target\n",
        "        cum_loss_source += loss_s.item()\n",
        "        ### For NLLLoss\n",
        "        # loss_source = criterion(m(out_source), y_source)\n",
        "        # loss_target = criterion(m(out_target), y_source)\n",
        "        # loss_s = loss_source + loss_target\n",
        "        # cum_loss_source += loss_s.item()\n",
        "\n",
        "        # Compute accuracy\n",
        "        cum_accuracy_source += compute_accuracy(out_source, y_source)\n",
        "        cum_accuracy_source += compute_accuracy(out_target, y_source)\n",
        "\n",
        "        # Set optimizers to zero_grad\n",
        "        optimizer_features.zero_grad()\n",
        "        optimizer_source.zero_grad()\n",
        "        optimizer_target.zero_grad()\n",
        "\n",
        "        # Backward\n",
        "        loss_s.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer_features.step()\n",
        "        optimizer_source.step()\n",
        "        optimizer_target.step()\n",
        "\n",
        "        ### UDA ###\n",
        "        # Extract features and compute output - source\n",
        "        features_source = features_model(x_source)\n",
        "        out_source_s = classifier_source(features_source)\n",
        "        out_target_s = classifier_target(features_source)\n",
        "\n",
        "        # Extract features and compute output - target\n",
        "        features_target = features_model(x_target)\n",
        "        out_source_t = classifier_source(features_target)\n",
        "        out_target_t = classifier_target(features_target)\n",
        "        \n",
        "        # Compute loss - Only on source!!!\n",
        "        loss_source = criterion(out_source_s, y_source)\n",
        "        loss_target = criterion(out_target_s, y_source)\n",
        "        loss_t = loss_source + loss_target\n",
        "        ### For NLLLoss\n",
        "        # loss_source = criterion(m(out_source_s), y_source)\n",
        "        # loss_target = criterion(m(out_target_s), y_source)\n",
        "        # loss_t = loss_source + loss_target\n",
        "\n",
        "        # Compute discrepancy loss on target\n",
        "        d = discrepancy(out_source_t, out_target_t)\n",
        "\n",
        "        # Compute total loss\n",
        "        loss: torch.Tensor = loss_t - d\n",
        "        cum_loss_target += loss.item()\n",
        "\n",
        "        # Compute accuracy - Can we compute accuracy on target since it is unsupervised? \n",
        "        cum_accuracy_target += compute_accuracy(out_source_t, y_target)\n",
        "        cum_accuracy_target += compute_accuracy(out_target_t, y_target)\n",
        "\n",
        "        # Backward the toal loss of source and target\n",
        "        loss.backward()\n",
        "\n",
        "        # Set optimizers to zero_grad\n",
        "        optimizer_source.zero_grad()\n",
        "        optimizer_target.zero_grad()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer_source.step()\n",
        "        optimizer_target.step()\n",
        "        \n",
        "        # Update n_samples\n",
        "        n_samples += x_source.shape[0]\n",
        "        n_samples += x_target.shape[0]\n",
        "        \n",
        "        ### Train features model for k steps ###\n",
        "        for i in range(k):\n",
        "            # Extract features and compute output \n",
        "            features_target = features_model(x_target)\n",
        "            out_source_t = classifier_source(features_target)\n",
        "            out_target_t = classifier_target(features_target)\n",
        "\n",
        "            # Compute loss\n",
        "            loss_discrepancy: torch.Tensor = discrepancy_fn(out_source_t, out_target_t)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer_features.zero_grad()\n",
        "\n",
        "            # Backpropagate\n",
        "            loss_discrepancy.backward()\n",
        "            \n",
        "            # Step optimizer\n",
        "            optimizer_features.step()\n",
        "\n",
        "    return {\n",
        "        'train/discrepancy' : d.cpu().detach().numpy(),\n",
        "        'train/loss_features' : loss_discrepancy.cpu().detach().numpy(),\n",
        "        'train/loss_source': cum_loss_source / n_samples,\n",
        "        'train/loss_target': cum_loss_target / n_samples,\n",
        "        'train/accuracy_source': cum_accuracy_source / n_samples,\n",
        "        'train/accuracy_target': cum_accuracy_target / n_samples,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9gVaNcOPJuO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test_mcd_one_step(\n",
        "        feature_model: nn.Module, \n",
        "        classifiers: Tuple[nn.Module, nn.Module],\n",
        "        criterion: nn.Module,\n",
        "        dataset: DataLoader,\n",
        "        device: str = DEFAULT_DEVICE,\n",
        "    ) -> dict:\n",
        "    ### For NLLLoss\n",
        "    # m = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    # Extract classifiers\n",
        "    classifier_source, classifier_target = classifiers\n",
        "\n",
        "    # Set models to eval mode\n",
        "    feature_model.eval()\n",
        "    classifier_source.eval()\n",
        "    classifier_target.eval()\n",
        "\n",
        "    # Initialize cumulative variables\n",
        "    cum_loss = 0\n",
        "    cum_accuracy_source = 0\n",
        "    cum_accuracy_target = 0\n",
        "    cum_accuracy = 0\n",
        "    n_samples = 0\n",
        "\n",
        "    # Test loop\n",
        "    for x, y in dataset:\n",
        "        # Move to device\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # Extract features and compute output\n",
        "        features = feature_model(x)\n",
        "        out_source = classifier_source(features)\n",
        "        out_target = classifier_target(features)\n",
        "        out = out_source + out_target\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(out, y)\n",
        "        ### For NLLLoss\n",
        "        # loss = criterion(m(out), y)\n",
        "        cum_loss += loss.item()\n",
        "\n",
        "        # Compute accuracy\n",
        "        cum_accuracy_source += compute_accuracy(out_source, y)\n",
        "        cum_accuracy_target += compute_accuracy(out_target, y)\n",
        "        cum_accuracy += compute_accuracy(out, y)\n",
        "\n",
        "        # Update n_samples\n",
        "        n_samples += x.shape[0]\n",
        "\n",
        "    return {\n",
        "        'eval/loss': cum_loss / n_samples,\n",
        "        'eval/accuracy_source': cum_accuracy_source / n_samples,\n",
        "        'eval/accuracy_target': cum_accuracy_target / n_samples,\n",
        "        'eval/accuracy': cum_accuracy / n_samples,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRNdR0jWjmzg"
      },
      "outputs": [],
      "source": [
        "def train_mcd(\n",
        "        feature_model: nn.Module, \n",
        "        classifiers: Tuple[nn.Module, nn.Module],\n",
        "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.Optimizer, torch.optim.Optimizer], \n",
        "        schedulers: Tuple[torch.optim.lr_scheduler._LRScheduler, torch.optim.lr_scheduler._LRScheduler, torch.optim.lr_scheduler._LRScheduler],\n",
        "        criterion: nn.Module, \n",
        "        discrepancy_fn: Callable,\n",
        "        train_datasets: Tuple[DataLoader, DataLoader],\n",
        "        eval_dataset: DataLoader,\n",
        "        n_epochs: int = DEFAULT_EPOCHS,\n",
        "        k: int = DEFAULT_K_STEPS,\n",
        "        device: str = DEFAULT_DEVICE,\n",
        "        save_name: str = \"UDA_MCD_P_RW\",\n",
        "    ) -> float:\n",
        "    # Extract classifiers\n",
        "    classifier_source, classifier_target = classifiers\n",
        "\n",
        "    # Move to device\n",
        "    feature_model.to(device)\n",
        "    classifier_source.to(device)\n",
        "    classifier_target.to(device)\n",
        "\n",
        "    # Extract schedulers\n",
        "    scheduler_features, scheduler_source, scheduler_target = schedulers\n",
        "\n",
        "    # Initialize variables\n",
        "    best_accuracy = 0\n",
        "    best_model_f = None\n",
        "    best_model_s = None\n",
        "    best_model_t = None\n",
        "\n",
        "    # Start timer\n",
        "    start_timer = time.time()\n",
        "\n",
        "    # Store metrics \n",
        "    metrics_visualize = {\n",
        "        'train/discrepancy': [],\n",
        "        'train/loss_features': [],\n",
        "        'train/loss_source': [],\n",
        "        'train/loss_target': [],\n",
        "        'train/accuracy_source': [],\n",
        "        'train/accuracy_target': [],\n",
        "        'eval/loss': [],\n",
        "        'eval/accuracy': [],\n",
        "        'eval/accuracy_source': [],\n",
        "        'eval/accuracy_target': [],\n",
        "    }\n",
        "\n",
        "    # Train loop\n",
        "    for epoch in range(n_epochs):\n",
        "        # Train\n",
        "        train_metrics = train_mcd_one_step(\n",
        "            features_model=feature_model, \n",
        "            classifiers=classifiers, \n",
        "            optimizers=optimizers, \n",
        "            criterion=criterion, \n",
        "            discrepancy_fn=discrepancy_fn,\n",
        "            datasets=train_datasets, \n",
        "            k=k,\n",
        "            device=device,\n",
        "        )\n",
        "        # Test\n",
        "        test_metrics = test_mcd_one_step(\n",
        "            feature_model=feature_model, \n",
        "            classifiers=classifiers, \n",
        "            criterion=criterion,\n",
        "            dataset=eval_dataset, \n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        # Put together metrics\n",
        "        metrics = {**train_metrics, **test_metrics}\n",
        "\n",
        "        # wandb log\n",
        "        wandb.log(metrics)\n",
        "\n",
        "        # Save best model\n",
        "        if test_metrics['eval/accuracy'] > best_accuracy:\n",
        "            best_accuracy = test_metrics['eval/accuracy']\n",
        "\n",
        "            best_model_f = copy.deepcopy(feature_model)\n",
        "            best_model_s = copy.deepcopy(classifier_source)\n",
        "            best_model_t = copy.deepcopy(classifier_target)\n",
        "            \n",
        "            torch.save(best_model_f.state_dict(), os.path.join(LOG_PATH_MODELS, save_name+\"_features_extractor.pth\"))\n",
        "            torch.save(best_model_s.state_dict(), os.path.join(LOG_PATH_MODELS, save_name+\"_source.pth\"))\n",
        "            torch.save(best_model_t.state_dict(), os.path.join(LOG_PATH_MODELS, save_name+\"_target.pth\"))\n",
        "\n",
        "        # Log metrics\n",
        "        opt_f, opt_s, opt_t = optimizers\n",
        "        make_log_print(\"Train\", (epoch+1, n_epochs), time.time() - start_timer, metrics, lr=(round(opt_f.param_groups[0]['lr'], 6), round(opt_s.param_groups[0]['lr'], 6), round(opt_t.param_groups[0]['lr'], 6)))\n",
        "\n",
        "        # Store metrics\n",
        "        for key in metrics_visualize.keys():\n",
        "            metrics_visualize[key].append(metrics[key])\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler_features.step()\n",
        "        scheduler_source.step()\n",
        "        scheduler_target.step()\n",
        "\n",
        "    # Plot metrics\n",
        "    visualize_results(\n",
        "        feature_model=best_model_f,\n",
        "        classifier_model_s=best_model_s,\n",
        "        classifier_model_t=best_model_t,\n",
        "        eval_dataset=eval_dataset,\n",
        "        metrics=metrics_visualize, \n",
        "        device=device, \n",
        "        save_path=os.path.join(LOG_PATH_IMAGES, save_name))\n",
        "\n",
        "    return best_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9zle4mdjmzg"
      },
      "source": [
        "### $P \\rightarrow RW$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gyk4z-g3LoG"
      },
      "outputs": [],
      "source": [
        "feature_model = FeatureModel(resnet_version='resnet18')\n",
        "model_s = Predictor(input_size=feature_model.output_size)\n",
        "model_t = Predictor(input_size=feature_model.output_size)\n",
        "\n",
        "opt_f = get_optimizer(feature_model, optimizer='sgd', lr=1e-2)\n",
        "opt_s = get_optimizer(model_s, optimizer='sgd', lr=1e-1)\n",
        "opt_t = get_optimizer(model_t, optimizer='sgd', lr=1e-1)\n",
        "\n",
        "scheduler_f = get_scheduler(opt_f)\n",
        "scheduler_s = get_scheduler(opt_s)\n",
        "scheduler_t = get_scheduler(opt_t)\n",
        "\n",
        "criterion = get_criterion(criterion='cross_entropy')\n",
        "\n",
        "# Initialize wandb\n",
        "experiment_name = 'UDA_P_RW'\n",
        "\n",
        "wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=DEFAULT_WANDB_PROJECT_NAME, \n",
        "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    name=experiment_name, \n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        \"feature_model\": feature_model.__class__.__name__,\n",
        "        \"source_model\": model_s.__class__.__name__,\n",
        "        \"target_model\": model_t.__class__.__name__,\n",
        "        \"features_optimizer\": opt_f.__class__.__name__,\n",
        "        \"source_optimizer\": opt_s.__class__.__name__,\n",
        "        \"target_optimizer\": opt_t.__class__.__name__,\n",
        "        \"features_scheduler\": scheduler_f.__class__.__name__,\n",
        "        \"source_scheduler\": scheduler_s.__class__.__name__,\n",
        "        \"target_scheduler\": scheduler_t.__class__.__name__,\n",
        "        \"criterion\": criterion.__class__.__name__,\n",
        "        \"training_type\": \"UDA P-RW\",\n",
        "        \"training_dataset\": \"Products Supervised + RealWorld Unsupervised\",\n",
        "        \"evaluation_dataset\": \"RealWorld\",\n",
        "        \"learning_rate_feature_model\": opt_f.param_groups[0]['lr'],\n",
        "        \"learning_rate_source_model\": opt_s.param_groups[0]['lr'],\n",
        "        \"learning_rate_target_model\": opt_t.param_groups[0]['lr'],\n",
        "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
        "        \"epochs\": DEFAULT_EPOCHS,\n",
        "    }\n",
        ")\n",
        "\n",
        "accuracy_uda_p_rw = train_mcd(\n",
        "    feature_model=feature_model,\n",
        "    classifiers=(model_s, model_t),\n",
        "    optimizers=(opt_f, opt_s, opt_t),\n",
        "    schedulers=(scheduler_f, scheduler_s, scheduler_t),\n",
        "    criterion=criterion,\n",
        "    discrepancy_fn=discrepancy,\n",
        "    train_datasets=(train_loader_products, train_loader_reals),\n",
        "    eval_dataset=test_loader_reals,\n",
        "    n_epochs=DEFAULT_EPOCHS,\n",
        "    k=DEFAULT_K_STEPS,\n",
        "    save_name=experiment_name\n",
        ")\n",
        "\n",
        "# Close wandb\n",
        "wandb.finish()\n",
        "\n",
        "# Delete variables\n",
        "del feature_model, model_s, model_t, opt_f, opt_s, opt_t, scheduler_f, scheduler_s, scheduler_t, criterion, experiment_name\n",
        "\n",
        "# Empty cache\n",
        "if DEFAULT_DEVICE.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy UDA P-RW: {:.2f}%\".format(accuracy_uda_p_rw*100))\n",
        "print(\"Gain UDA P-RW: {:.2f}%\".format((accuracy_uda_p_rw - 0.807)*100))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "43hf5D-u0E4P"
      },
      "source": [
        "#### Results\n",
        "\n",
        "Here we report the results of the baseline on the $P \\rightarrow RW$ direction.\n",
        "\n",
        "##### Final results\n",
        "\n",
        "<center>\n",
        "\n",
        "| | Accuracy | Loss |\n",
        "| :---: | :---: | :---: |\n",
        "| Train source (supervised) | 88.9 % | 0.002 |\n",
        "| Train target (unsupervised) | 75.4 % | 0.001 |\n",
        "| Test source | 87.5 % | 0.002 |\n",
        "| Test target | 87.7 % | 0.002 |\n",
        "| Test overall | 88.0 % | 0.002 |\n",
        "\n",
        "</center>\n",
        "\n",
        "##### Metrics\n",
        "\n",
        "<center>\n",
        "\n",
        "| | precision | recall | f1-score | support |\n",
        "| :---: | :---: | :---: | :---: | :---: |\n",
        "| backpack | 0.95 | 0.88 | 0.91 | 24 |\n",
        "| bookcase | 0.91 | 1.00 | 0.95 | 21 |\n",
        "| car jack | 0.90 | 0.82 | 0.86 | 22 |\n",
        "| comb | 0.79 | 0.88 | 0.83 | 17 |\n",
        "| crown | 1.00 | 0.95 | 0.97 | 19 |\n",
        "| file cabinet | 1.00 | 1.00 | 1.00 | 21 |\n",
        "| flat iron | 0.95 | 0.86 | 0.90 | 21 |\n",
        "| game controller | 0.94 | 1.00 | 0.97 | 17 |\n",
        "| glasses | 0.85 | 0.81 | 0.83 | 27 |\n",
        "| helicopter | 0.70 | 0.94 | 0.80 | 17 |\n",
        "| ice skates | 0.70 | 0.86 | 0.78 | 22 |\n",
        "| letter tray | 0.88 | 1.00 | 0.93 | 21 |\n",
        "| monitor | 0.76 | 0.87 | 0.81 | 15 |\n",
        "| mug | 1.00 | 0.81 | 0.89 | 21 |\n",
        "| network switch | 0.88 | 0.94 | 0.91 | 16 |\n",
        "| over-ear headphones | 0.89 | 0.85 | 0.87 | 20 |\n",
        "| pen | 0.95 | 0.95 | 0.95 | 19 |\n",
        "| purse | 0.88 | 0.88 | 0.88 | 17 |\n",
        "| stroller | 0.88 | 0.65 | 0.75 | 23 |\n",
        "| stand mixer | 0.88 | 0.75 | 0.81 | 20 |\n",
        "| | | | | |\n",
        "| accuracy | | | 0.88 | 400 |\n",
        "| macro avg | 0.89 | 0.88 | 0.88 | 400 |\n",
        "| weighted avg | 0.89 | 0.88 | 0.88 | 400 |\n",
        "\n",
        "</center>\n",
        "\n",
        "##### Loss and Accuracy curves\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1S3MAoINzyNJ8uZHAtjc7hPxqUd5gRKog\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1fBV4dWJDjCSl8yW-jg_YV9DDbcTmBim8\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1e5_Y1drSnguFs_b4M5RJNBCQ7sMw0AAv\" width=400px>\n",
        "    <figcaption align = \"center\"><b>Loss & Accuracy & Discrepancy(features only)</b></figcaption>\n",
        "</p>\n",
        "\n",
        "##### Confusion Matrix\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=151JLK_-ohBakq8jzkAId27S4G_Wpog8g\" width=600px>\n",
        "    <figcaption align = \"center\"><b>Confusion Matrix</b></figcaption>\n",
        "</p>\n",
        "\n",
        "##### t-SNE\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1uk6_7Dx_AN-rrHrGgd7lXjTUD6Fld_hu\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1gZafwwJh6Dv7MaPef45CdyUzikPCsWic\" width=400px>\n",
        "    <figcaption align = \"center\"><b>t-SNE on Source & Target</b></figcaption>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Discussion : $P \\rightarrow RW$\n",
        "As we can see from the results we had a lot of improvements for the performances, in particular we had 7.2 % of gain. Surely the t-SNE representation and the confusion matrix are more precise then the respective plots of the baseline.\n",
        "\n",
        "As we can see from the Figure below the UDA confusion matrix, on the right, is more sparse around the diagonal. This means that more classes have been correctly classified. \n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1ccL_83f5IGKVjBTQfZ_FWy69c7eryHQK\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=151JLK_-ohBakq8jzkAId27S4G_Wpog8g\" width=400px>\n",
        "    <figcaption align = \"center\"><b>Confusion matrix Baseline and MCD</b></figcaption>\n",
        "</p>\n",
        "\n",
        "The t-SNE representations, instead, can show us that the extracted features of the classes in UDA are more compact with respect to the classes features from the baseline model. The representation from the target classifier has more outliers from the source classfier. \n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1MePvXiItwbH8fEM7VrL0hm8IZdHoN_Yf\" width=300px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1uk6_7Dx_AN-rrHrGgd7lXjTUD6Fld_hu\" width=300px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1gZafwwJh6Dv7MaPef45CdyUzikPCsWic\" width=300px>\n",
        "    <figcaption align = \"center\"><b>t-SNE Baseline source only and MCD source and target classifier</b></figcaption>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXwEglWPjmzh"
      },
      "source": [
        "### $RW \\rightarrow P$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpFxLx4xjmzh"
      },
      "outputs": [],
      "source": [
        "feature_model = FeatureModel(resnet_version='resnet18')\n",
        "model_s = Predictor(input_size=feature_model.output_size)\n",
        "model_t = Predictor(input_size=feature_model.output_size)\n",
        "\n",
        "opt_f = get_optimizer(feature_model, optimizer='sgd', lr=1e-2)\n",
        "opt_s = get_optimizer(model_s, optimizer='sgd', lr=1e-1)\n",
        "opt_t = get_optimizer(model_t, optimizer='sgd', lr=1e-1)\n",
        "\n",
        "scheduler_f = get_scheduler(opt_f)\n",
        "scheduler_s = get_scheduler(opt_s)\n",
        "scheduler_t = get_scheduler(opt_t)\n",
        "\n",
        "criterion = get_criterion(criterion='cross_entropy')\n",
        "\n",
        "# Initialize wandb\n",
        "experiment_name = 'UDA_RW_P'\n",
        "\n",
        "wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=DEFAULT_WANDB_PROJECT_NAME, \n",
        "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    name=experiment_name, \n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        \"feature_model\": feature_model.__class__.__name__,\n",
        "        \"source_model\": model_s.__class__.__name__,\n",
        "        \"target_model\": model_t.__class__.__name__,\n",
        "        \"features_optimizer\": opt_f.__class__.__name__,\n",
        "        \"source_optimizer\": opt_s.__class__.__name__,\n",
        "        \"target_optimizer\": opt_t.__class__.__name__,\n",
        "        \"features_scheduler\": scheduler_f.__class__.__name__,\n",
        "        \"source_scheduler\": scheduler_s.__class__.__name__,\n",
        "        \"target_scheduler\": scheduler_t.__class__.__name__,\n",
        "        \"criterion\": criterion.__class__.__name__,\n",
        "        \"training_type\": \"UDA RW-P\",\n",
        "        \"training_dataset\": \"RealWorld Supervised + Products Unsupervised\",\n",
        "        \"evaluation_dataset\": \"Products\",\n",
        "        \"learning_rate_feature_model\": opt_f.param_groups[0]['lr'],\n",
        "        \"learning_rate_source_model\": opt_s.param_groups[0]['lr'],\n",
        "        \"learning_rate_target_model\": opt_t.param_groups[0]['lr'],\n",
        "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
        "        \"epochs\": DEFAULT_EPOCHS,\n",
        "    }\n",
        ")\n",
        "\n",
        "accuracy_uda_rw_p = train_mcd(\n",
        "    feature_model=feature_model,\n",
        "    classifiers=(model_s, model_t),\n",
        "    optimizers=(opt_f, opt_s, opt_t),\n",
        "    schedulers=(scheduler_f, scheduler_s, scheduler_t),\n",
        "    criterion=criterion,\n",
        "    train_datasets=(train_loader_reals, train_loader_products),\n",
        "    eval_dataset=test_loader_products,\n",
        "    n_epochs=DEFAULT_EPOCHS,\n",
        "    k=DEFAULT_K_STEPS,\n",
        "    save_name=experiment_name\n",
        ")\n",
        "\n",
        "# Close wandb\n",
        "wandb.finish()\n",
        "\n",
        "# Delete variables\n",
        "del feature_model, model_s, model_t, opt_f, opt_s, opt_t, scheduler_f, scheduler_s, scheduler_t, criterion, experiment_name\n",
        "\n",
        "# Empty cache\n",
        "if DEFAULT_DEVICE.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy UDA RW-P: {:.2f}%\".format(accuracy_uda_rw_p*100))\n",
        "print(\"Gain UDA RW-P: {:.2f}%\".format((accuracy_uda_rw_p - 0.922)*100))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results\n",
        "\n",
        "Here we report the results of the baseline on the $P \\rightarrow RW$ direction.\n",
        "\n",
        "##### Final results\n",
        "\n",
        "<center>\n",
        "\n",
        "| | Accuracy | Loss |\n",
        "| :---: | :---: | :---: |\n",
        "| Train source (supervised) | 89.89 % | 0.001 |\n",
        "| Train target (unsupervised) | 86.3 % | 0.001 |\n",
        "| Test source | 96.3 % | 0.001 |\n",
        "| Test target | 96.3 % | 0.001 |\n",
        "| Test overall | 96.5% | 0.001 |\n",
        "\n",
        "</center>\n",
        "\n",
        "##### Metrics\n",
        "\n",
        "<center>\n",
        "\n",
        "| | precision | recall | f1-score | support |\n",
        "| :---: | :---: | :---: | :---: | :---: |\n",
        "| backpack | 0.96 | 1.00 | 0.98 | 24 |\n",
        "| bookcase | 1.00 | 1.00 | 1.00 | 17 |\n",
        "| car jack | 1.00 | 0.95 | 0.97 | 19 |\n",
        "| comb | 0.97 | 1.00 | 0.98 | 29 |\n",
        "| crown | 1.00 | 1.00 | 1.00 | 19 |\n",
        "| file cabinet | 0.94 | 1.00 | 0.97 | 15 |\n",
        "| flat iron | 0.94 | 1.00 | 0.97 | 16 |\n",
        "| game controller | 0.90 | 0.95 | 0.92 | 19 |\n",
        "| glasses | 0.94 | 0.89 | 0.91 | 18 |\n",
        "| helicopter | 1.00 | 0.90 | 0.95 | 21 |\n",
        "| ice skates | 1.00 | 1.00 | 1.00 | 17 |\n",
        "| letter tray | 1.00 | 1.00 | 1.00 | 19 |\n",
        "| monitor | 1.00 | 0.96 | 0.98 | 24 |\n",
        "| mug | 0.95 | 0.95 | 0.95 | 20 |\n",
        "| network switch | 0.92 | 0.83 | 0.87 | 29 |\n",
        "| over-ear headphones | 1.00 | 1.00 | 1.00 | 20 |\n",
        "| pen | 0.91 | 0.95 | 0.93 | 21 |\n",
        "| purse | 1.00 | 1.00 | 1.00 | 20 |\n",
        "| stroller | 1.00 | 1.00 | 1.00 | 17 |\n",
        "| stand mixer | 0.89 | 1.00 | 0.94 | 16 |\n",
        "| | | | | |\n",
        "| accuracy | | | 0.96 | 400 |\n",
        "| macro avg | 0.97 | 0.97 | 0.97 | 400 |\n",
        "| weighted avg | 0.97 | 0.96 | 0.96 | 400 |\n",
        "\n",
        "</center>\n",
        "\n",
        "##### Loss and Accuracy curves\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=18mD0Q0b3DgkqPWbxOp0UwIrRyqKHrbBY\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1o6itff12B8edlB_Brk3BI_RLTdtfTtan\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=17-tCeYqB7IZ9IBAgh7JqIAWOQcOcIVv0\" width=400px>\n",
        "    <figcaption align = \"center\"><b>Loss & Accuracy & Discrepancy(features only)</b></figcaption>\n",
        "</p>\n",
        "\n",
        "##### Confusion Matrix\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1Zo2TUDgOj1wd8SF8fdsuUl6bWgBaBGOL\" width=600px>\n",
        "    <figcaption align = \"center\"><b>Confusion Matrix</b></figcaption>\n",
        "</p>\n",
        "\n",
        "##### t-SNE\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1IMQ-p3TLz_A8Fj6eMaZARdQG2QTteaJA\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1PJYjGwy6A08NAmCn7e3aEvF3byaUqf6J\" width=400px>\n",
        "    <figcaption align = \"center\"><b>t-SNE on Source & Target</b></figcaption>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Discussion : $RW \\rightarrow P$\n",
        "\n",
        "As we can see from the results we had a lot of improvements for the performances. Indeed we almost reached the ideal upperbound. Despite of the fact that the t-SNE representation and the confusion matrix of the baseline were already good, with the UDA component we were able to reach greater performances. The accuracy achieves 96.5%, which is only a 0.07% difference to the upperbound. \n",
        "\n",
        "The final confusion matrix of the UDA model is almost perfect and is quite close to be an identity matrix, which tells us that the errors were only on few classes. In fact, only 8 classes over 20 do not have 100% of accuracy. Despite the fact that these calsses do not reach the maximum for the accuracy, they have a very high value, and only 2 of them were under the 90% of accuracy. \n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1AjQiAWXVMXEhjg97MWfC_rhceUeBrqrO\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1Zo2TUDgOj1wd8SF8fdsuUl6bWgBaBGOL\" width=400px>\n",
        "    <figcaption align = \"center\"><b>Confusion matrix Baseline and MCD</b></figcaption>\n",
        "</p>\n",
        "\n",
        "As for the confusion matrix, also the t-SNE representations are almost perfect. Indeed, the classes features extracted from the classifiers of the UDA component are quite perfectly divided, with only very few outliers.\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1myFwRjs6_xVE1op-KSdD34DGbXEInupD\" width=300px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1IMQ-p3TLz_A8Fj6eMaZARdQG2QTteaJA\" width=300px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1PJYjGwy6A08NAmCn7e3aEvF3byaUqf6J\" width=300px>\n",
        "    <figcaption align = \"center\"><b>t-SNE Baseline and MCD source and target classifier</b></figcaption>\n",
        "</p>\n",
        "\n",
        "The $RW \\rightarrow P$ direction with this UDA method has performed very well, almost reaching the upperbound. This makes very difficult to bring some improvements, because the margin is very little to have some better results.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JMyRA41Njmzh"
      },
      "source": [
        "### Final Discussion\n",
        "\n",
        "Here are reported the final gains for $P \\rightarrow RW$ and $RW \\rightarrow P$:\n",
        "\n",
        "<center>\n",
        "\n",
        "| | Accuracy Baseline | Accuracy UDA | Gain | Maximum Gain |\n",
        "| :---: | :---: | :---: |  :---: |  :---: |\n",
        "| $P \\rightarrow RW$ | 80.7 % | 88.0 % | 7.2 % | 9.8 % |\n",
        "| $RW \\rightarrow P$ | 92.2 % | 96.5 % | 4.3 % | 5.1 % |\n",
        "\n",
        "</center>\n",
        "\n",
        "As said before the high accuracy on $RW \\rightarrow P$ makes very complicated to have more improvements, while for $P \\rightarrow RW$ is more easy. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3H2St8XlPJuQ"
      },
      "source": [
        "## 4) Improvements proposed\n",
        "In the following section we propose an improvement for the MCD method and some other additional ideas for the MCD that we tried but they reach the same performaces of the MCD. In the first part there will be the explanation of the first improvement with the results, while in the second part the other ideas."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Improvement 1: Supervised training in the features generator\n",
        "\n",
        "In the paper of the previous approach, the authors were using three steps in the training, we recap them here:\n",
        "\n",
        "- **Step A** : train in a superised way on the source domain the Feature Extractor and both Classifiers\n",
        "- **Step B** : train in an unsupervised way on the target domain\n",
        "    - Step B.1 : Fix the Generator and train in an unsupervised way the 2 Classifiers on the target domain\n",
        "    - Step B.2 : Fix the Classifiers and train in an unsupervised way the Generator on the target domain\n",
        "\n",
        "What we though was missing is a supervised training on the features generator, taking into account not only the two distinct predictions of the classifiers, but also a combined prediction of the two.\n",
        "\n",
        "After step B we propose to add another step, we call it **Step C**, in which we will do the following procedure:\n",
        "- Extract the features from an image of the source domain\n",
        "- Compute the predictions with the classifier of the source domain and the classifier of the target domain, respectively called `out_source_s` and `out_target_s`\n",
        "- Compute the combined prediction : `out = out_source_s + out_target_s`\n",
        "- Compute the CrossEntropy Loss between the combined output and the respectively label of the source domain\n",
        "- Update weights of the Features Extractor with the Loss just computed\n",
        "\n",
        "For this improvement the only function we modifiy is the `train_mcd_one_step()` in which we add **Step C**. For the test and the loop training functions we keep the same of the MCD method. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSPXJmjUPJuT"
      },
      "outputs": [],
      "source": [
        "def train_mcd_one_step(\n",
        "        features_model: nn.Module, \n",
        "        classifiers: Tuple[nn.Module, nn.Module],\n",
        "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.Optimizer], \n",
        "        criterion: nn.Module, \n",
        "        datasets: Tuple[DataLoader, DataLoader],\n",
        "        epoch: int,\n",
        "        k: int = DEFAULT_K_STEPS,\n",
        "        device: str = DEFAULT_DEVICE,\n",
        "    ) -> Tuple[nn.Module, nn.Module, nn.Module]:\n",
        "    ### For NLLLoss\n",
        "    # m = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    # Extract classifiers\n",
        "    classifier_source, classifier_target = classifiers\n",
        "\n",
        "    # Extract optimizers\n",
        "    optimizer_features, optimizer_source, optimizer_target = optimizers\n",
        "\n",
        "    # Extract datasets\n",
        "    train_loader_source, train_loader_target = datasets\n",
        "\n",
        "    # Set models to train mode\n",
        "    features_model.train()\n",
        "    classifier_source.train()\n",
        "    classifier_target.train()\n",
        "    \n",
        "    # Initialize cumulative variables\n",
        "    cum_loss_source = 0\n",
        "    cum_loss_target = 0\n",
        "    cum_accuracy_source = 0\n",
        "    cum_accuracy_target = 0\n",
        "    n_samples = 0\n",
        "\n",
        "    # Train loop    \n",
        "    for (x_source, y_source), (x_target, y_target) in zip(train_loader_source, train_loader_target):\n",
        "        # Move to device\n",
        "        x_source, y_source = x_source.to(device), y_source.to(device)\n",
        "        x_target, y_target = x_target.to(device), y_target.to(device)\n",
        "\n",
        "        # Extract features and compute output\n",
        "        features_source = features_model(x_source)\n",
        "        out_source = classifier_source(features_source)\n",
        "        out_target = classifier_target(features_source)\n",
        "\n",
        "        # Compute loss\n",
        "        loss_source = criterion(out_source, y_source)\n",
        "        loss_target = criterion(out_target, y_source)\n",
        "        loss_s = loss_source + loss_target\n",
        "        cum_loss_source += loss_s.item()\n",
        "        ### For NLLLoss\n",
        "        # loss_source = criterion(m(out_source), y_source)\n",
        "        # loss_target = criterion(m(out_target), y_source)\n",
        "        # loss_s = loss_source + loss_target\n",
        "        # cum_loss_source += loss_s.item()\n",
        "\n",
        "        # Compute accuracy\n",
        "        cum_accuracy_source += compute_accuracy(out_source, y_source)\n",
        "        cum_accuracy_source += compute_accuracy(out_target, y_source)\n",
        "\n",
        "        # Set optimizers to zero_grad\n",
        "        optimizer_features.zero_grad()\n",
        "        optimizer_source.zero_grad()\n",
        "        optimizer_target.zero_grad()\n",
        "\n",
        "        # Backward\n",
        "        loss_s.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer_features.step()\n",
        "        optimizer_source.step()\n",
        "        optimizer_target.step()\n",
        "\n",
        "        ### UDA ###\n",
        "        # Extract features and compute output - source\n",
        "        features_source = features_model(x_source)\n",
        "        out_source_s = classifier_source(features_source)\n",
        "        out_target_s = classifier_target(features_source)\n",
        "\n",
        "        # Extract features and compute output - target\n",
        "        features_target = features_model(x_target)\n",
        "        out_source_t = classifier_source(features_target)\n",
        "        out_target_t = classifier_target(features_target)\n",
        "        \n",
        "        # Compute loss - Only on source!!!\n",
        "        loss_source = criterion(out_source_s, y_source)\n",
        "        loss_target = criterion(out_target_s, y_source)\n",
        "        loss_t = loss_source + loss_target\n",
        "        ### For NLLLoss\n",
        "        # loss_source = criterion(m(out_source_s), y_source)\n",
        "        # loss_target = criterion(m(out_target_s), y_source)\n",
        "        # loss_t = loss_source + loss_target\n",
        "\n",
        "        # Compute discrepancy loss on target\n",
        "        d = discrepancy(out_source_t, out_target_t)\n",
        "\n",
        "        # Compute total loss\n",
        "        loss = loss_t - d\n",
        "        cum_loss_target += loss.item()\n",
        "\n",
        "        # Compute accuracy - Can we compute accuracy on target since it is unsupervised? \n",
        "        cum_accuracy_target += compute_accuracy(out_source_t, y_target)\n",
        "        cum_accuracy_target += compute_accuracy(out_target_t, y_target)\n",
        "\n",
        "        # Backward the toal loss of source and target\n",
        "        loss.backward()\n",
        "\n",
        "        # Set optimizers to zero_grad\n",
        "        optimizer_source.zero_grad()\n",
        "        optimizer_target.zero_grad()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer_source.step()\n",
        "        optimizer_target.step()\n",
        "        \n",
        "        # Update n_samples\n",
        "        n_samples += x_source.shape[0]\n",
        "        n_samples += x_target.shape[0]\n",
        "        \n",
        "        \n",
        "        for i in range(k):\n",
        "            # Extract features and compute output \n",
        "            features_target = features_model(x_target)\n",
        "            out_source_t = classifier_source(features_target)\n",
        "            out_target_t = classifier_target(features_target)\n",
        "\n",
        "            # Compute loss\n",
        "            loss_discrepancy = discrepancy(out_source_t, out_target_t)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer_features.zero_grad()\n",
        "\n",
        "            # Backpropagate\n",
        "            loss_discrepancy.backward()\n",
        "            \n",
        "            # Step optimizer\n",
        "            optimizer_features.step()\n",
        "\n",
        "        # Step C:\n",
        "        # Extract features and compute output \n",
        "        features_source = feature_model(x_source)\n",
        "        out_source_s = classifier_source(features_source)\n",
        "        out_target_s = classifier_target(features_source)\n",
        "\n",
        "        # Compute cobined output\n",
        "        out = out_source_s + out_target_s\n",
        "\n",
        "        # Compute loss\n",
        "        loss_CE = criterion(out, y_source)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer_features.zero_grad()\n",
        "\n",
        "        # Backpropagate\n",
        "        loss_CE.backward()\n",
        "        \n",
        "        # Step optimizer\n",
        "        optimizer_features.step()\n",
        "\n",
        "\n",
        "    return {\n",
        "        'train/discrepancy' : d.cpu().detach().numpy(),\n",
        "        'train/loss_features' : loss_discrepancy.cpu().detach().numpy(),\n",
        "        'train/loss_source': cum_loss_source / n_samples,\n",
        "        'train/loss_target': cum_loss_target / n_samples,\n",
        "        'train/accuracy_source': cum_accuracy_source / n_samples,\n",
        "        'train/accuracy_target': cum_accuracy_target / n_samples,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_mcd(\n",
        "        feature_model: nn.Module, \n",
        "        classifiers: Tuple[nn.Module, nn.Module],\n",
        "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.Optimizer, torch.optim.Optimizer], \n",
        "        schedulers: Tuple[torch.optim.lr_scheduler._LRScheduler, torch.optim.lr_scheduler._LRScheduler, torch.optim.lr_scheduler._LRScheduler],\n",
        "        criterion: nn.Module, \n",
        "        train_datasets: Tuple[DataLoader, DataLoader],\n",
        "        eval_dataset: DataLoader,\n",
        "        n_epochs: int = DEFAULT_EPOCHS,\n",
        "        k: int = DEFAULT_K_STEPS,\n",
        "        device: str = DEFAULT_DEVICE,\n",
        "        save_name: str = \"UDA_MCD_P_RW\",\n",
        "    ) -> Tuple[nn.Module, nn.Module, nn.Module]:\n",
        "    # Extract classifiers\n",
        "    classifier_source, classifier_target = classifiers\n",
        "\n",
        "    # Move to device\n",
        "    feature_model.to(device)\n",
        "    classifier_source.to(device)\n",
        "    classifier_target.to(device)\n",
        "\n",
        "    # Extract schedulers\n",
        "    scheduler_features, scheduler_source, scheduler_target = schedulers\n",
        "\n",
        "    # Initialize variables\n",
        "    best_accuracy = 0\n",
        "    best_model_f = None\n",
        "    best_model_s = None\n",
        "    best_model_t = None\n",
        "\n",
        "    # Start timer\n",
        "    start_timer = time.time()\n",
        "\n",
        "    # Store metrics \n",
        "    metrics_visualize = {\n",
        "        'train/discrepancy': [],\n",
        "        'train/loss_features': [],\n",
        "        'train/loss_source': [],\n",
        "        'train/loss_target': [],\n",
        "        'train/accuracy_source': [],\n",
        "        'train/accuracy_target': [],\n",
        "        'eval/loss': [],\n",
        "        'eval/accuracy': [],\n",
        "        'eval/accuracy_source': [],\n",
        "        'eval/accuracy_target': [],\n",
        "    }\n",
        "\n",
        "    # Train loop\n",
        "    for epoch in range(n_epochs):\n",
        "        # Train\n",
        "        train_metrics = train_mcd_one_step(\n",
        "            features_model=feature_model, \n",
        "            classifiers=classifiers, \n",
        "            optimizers=optimizers, \n",
        "            criterion=criterion, \n",
        "            datasets=train_datasets, \n",
        "            epoch=epoch,\n",
        "            k=k,\n",
        "            device=device,\n",
        "        )\n",
        "        # Test\n",
        "        test_metrics = test_mcd_one_step(\n",
        "            feature_model=feature_model, \n",
        "            classifiers=classifiers, \n",
        "            criterion=criterion,\n",
        "            dataset=eval_dataset, \n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        # Put together metrics\n",
        "        metrics = {**train_metrics, **test_metrics}\n",
        "\n",
        "        # wandb log\n",
        "        wandb.log(metrics)\n",
        "\n",
        "        # Save best model\n",
        "        if test_metrics['eval/accuracy'] > best_accuracy:\n",
        "            best_accuracy = test_metrics['eval/accuracy']\n",
        "\n",
        "            best_model_f = copy.deepcopy(feature_model)\n",
        "            best_model_s = copy.deepcopy(classifier_source)\n",
        "            best_model_t = copy.deepcopy(classifier_target)\n",
        "            \n",
        "            torch.save(best_model_f.state_dict(), os.path.join(LOG_PATH_MODELS, save_name+\"_features_extractor.pth\"))\n",
        "            torch.save(best_model_s.state_dict(), os.path.join(LOG_PATH_MODELS, save_name+\"_source.pth\"))\n",
        "            torch.save(best_model_t.state_dict(), os.path.join(LOG_PATH_MODELS, save_name+\"_target.pth\"))\n",
        "\n",
        "        # Log metrics\n",
        "        opt_f, opt_s, opt_t = optimizers\n",
        "        make_log_print(\"Train\", (epoch+1, n_epochs), time.time() - start_timer, metrics, lr=(round(opt_f.param_groups[0]['lr'], 6), round(opt_s.param_groups[0]['lr'], 6), round(opt_t.param_groups[0]['lr'], 6)))\n",
        "\n",
        "        # Store metrics\n",
        "        for key in metrics_visualize.keys():\n",
        "            metrics_visualize[key].append(metrics[key])\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler_features.step()\n",
        "        scheduler_source.step()\n",
        "        scheduler_target.step()\n",
        "\n",
        "    # Plot metrics\n",
        "    visualize_results(\n",
        "        feature_model=best_model_f,\n",
        "        classifier_model_s=best_model_s,\n",
        "        classifier_model_t=best_model_t,\n",
        "        eval_dataset=eval_dataset,\n",
        "        metrics=metrics_visualize, \n",
        "        device=device, \n",
        "        save_path=os.path.join(LOG_PATH_IMAGES, save_name))\n",
        "        \n",
        "    return best_accuracy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $P \\rightarrow RW$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTZxbnb-PJuU"
      },
      "outputs": [],
      "source": [
        "feature_model = FeatureModel(resnet_version='resnet18')\n",
        "model_s = Predictor(input_size=feature_model.output_size)\n",
        "model_t = Predictor(input_size=feature_model.output_size)\n",
        "\n",
        "opt_f = get_optimizer(feature_model, optimizer='sgd', lr=1e-2)\n",
        "opt_s = get_optimizer(model_s, optimizer='sgd', lr=1e-1)\n",
        "opt_t = get_optimizer(model_t, optimizer='sgd', lr=1e-1)\n",
        "\n",
        "scheduler_f = get_scheduler(opt_f)\n",
        "scheduler_s = get_scheduler(opt_s)\n",
        "scheduler_t = get_scheduler(opt_t)\n",
        "\n",
        "criterion = get_criterion(criterion='cross_entropy')\n",
        "\n",
        "# Initialize wandb\n",
        "experiment_name = 'UDA_P_RW_1'\n",
        "\n",
        "wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=DEFAULT_WANDB_PROJECT_NAME, \n",
        "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    name=experiment_name, \n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        \"feature_model\": feature_model.__class__.__name__,\n",
        "        \"source_model\": model_s.__class__.__name__,\n",
        "        \"target_model\": model_t.__class__.__name__,\n",
        "        \"features_optimizer\": opt_f.__class__.__name__,\n",
        "        \"source_optimizer\": opt_s.__class__.__name__,\n",
        "        \"target_optimizer\": opt_t.__class__.__name__,\n",
        "        \"features_scheduler\": scheduler_f.__class__.__name__,\n",
        "        \"source_scheduler\": scheduler_s.__class__.__name__,\n",
        "        \"target_scheduler\": scheduler_t.__class__.__name__,\n",
        "        \"criterion\": criterion.__class__.__name__,\n",
        "        \"training_type\": \"UDA P-RW Improvement 1\",\n",
        "        \"training_dataset\": \"Products Supervised + RealWorld Unsupervised\",\n",
        "        \"evaluation_dataset\": \"RealWorld\",\n",
        "        \"learning_rate_feature_model\": opt_f.param_groups[0]['lr'],\n",
        "        \"learning_rate_source_model\": opt_s.param_groups[0]['lr'],\n",
        "        \"learning_rate_target_model\": opt_t.param_groups[0]['lr'],\n",
        "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
        "        \"epochs\": 40,\n",
        "    }\n",
        ")\n",
        "\n",
        "accuracy_uda_p_rw_1 = train_mcd(\n",
        "    feature_model=feature_model,\n",
        "    classifiers=(model_s, model_t),\n",
        "    optimizers=(opt_f, opt_s, opt_t),\n",
        "    schedulers=(scheduler_f, scheduler_s, scheduler_t),\n",
        "    criterion=criterion,\n",
        "    train_datasets=(train_loader_products, train_loader_reals),\n",
        "    eval_dataset=test_loader_reals,\n",
        "    n_epochs=DEFAULT_EPOCHS + 10,\n",
        "    k=DEFAULT_K_STEPS,\n",
        "    save_name=experiment_name\n",
        ")\n",
        "\n",
        "# Close wandb\n",
        "wandb.finish()\n",
        "\n",
        "# Delete variables\n",
        "del feature_model, model_s, model_t, opt_f, opt_s, opt_t, scheduler_f, scheduler_s, scheduler_t, criterion, experiment_name\n",
        "\n",
        "# Empty cache\n",
        "if DEFAULT_DEVICE.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy UDA P-RW: {:.2f}%\".format(accuracy_uda_p_rw_1*100))\n",
        "print(\"Gain UDA P-RW: {:.2f}%\".format((accuracy_uda_p_rw_1 - accuracy_baseline_p_rw)*100))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Results \n",
        "\n",
        "<center>\n",
        "\n",
        "| | Accuracy | Loss |\n",
        "| :---: | :---: | :---: |\n",
        "| Train source (supervised) | 76.5 % | 0.001 |\n",
        "| Train target (unsupervised) | 91.2 % | 0.001 |\n",
        "| Test source | 88.3 % | 0.002 |\n",
        "| Test target | 88 % | 0.002 |\n",
        "| Test overall | 89% | 0.002 |\n",
        "\n",
        "</center>\n",
        "\n",
        "###### Metrics\n",
        "<center>\n",
        "\n",
        "| | precision | recall | f1-score |  support |\n",
        "| :---: | :---: | :---: | :---: | :---: |\n",
        "| backpack | 0.90 | 0.86 | 0.88 | 22 |\n",
        "| bookcase | 0.88 | 0.88 | 0.88 | 17 |\n",
        "| car jack | 0.68 | 1.00 | 0.81 | 17 |\n",
        "| comb | 0.88 | 0.85 | 0.87 | 27 |\n",
        "| crown | 0.88 | 0.94 | 0.91 | 16 |\n",
        "| file cabinet | 0.86 | 0.86 | 0.86 | 21 |\n",
        "| flat iron | 1.00 | 0.81 | 0.89 | 21 |\n",
        "| helicopter | 0.88 | 0.88 | 0.88 | 17 |\n",
        "| game controller | 0.90 | 0.95 | 0.92 | 19 |\n",
        "| glasses | 0.68 | 0.86 | 0.76 | 22 |\n",
        "| ice skates | 0.89 | 0.85 | 0.87 | 20 |\n",
        "| letter tray | 0.86 | 0.78 | 0.82 | 23 |\n",
        "| monitor | 0.95 | 1.00 | 0.98 | 21 |\n",
        "| mug | 1.00 | 1.00 | 1.00 | 21 |\n",
        "| network switch | 0.94 | 0.75 | 0.83 | 20 |\n",
        "| over-ear headphones | 0.88 | 0.93 | 0.90 | 15 |\n",
        "| pen | 0.94 | 1.00 | 0.97 | 17 |\n",
        "| purse | 0.95 | 0.88 | 0.91 | 24 |\n",
        "| stand mixer | 1.00 | 0.95 | 0.97 | 19 |\n",
        "| stroller | 1.00 | 0.86 | 0.92 | 21 |\n",
        "| | | | | |\n",
        "| accuracy | | | 0.89 | 400 |\n",
        "| macro avg | 0.90 | 0.89 | 0.89 | 400 |\n",
        "| weighted avg | 0.90 | 0.89 | 0.89 | 400 |\n",
        "\n",
        "</center>\n",
        "\n",
        "###### Loss and Accuracy curves\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=13e5QlZbba5XoL901SsMKedJkCfDXvMaP\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1T4HTrsFsEaXkrZG9xDGccrjTfJXX-c9U\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1_hs-IGUr5pnR4dXo4l5_5DMb4g_lsJTF\" width=400px>\n",
        "    <figcaption align = \"center\"><b>Loss & Accuracy & Discrepancy(features only)</b></figcaption>\n",
        "</p>\n",
        "\n",
        "###### Confusion Matrix\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1cKJVp0WDaRR3NnUrAB5DUpoT9axvZZOS\" width=600px>\n",
        "    <figcaption align = \"center\"><b>Confusion Matrix</b></figcaption>\n",
        "</p>\n",
        "\n",
        "###### t-SNE\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=101I23Ef_Uj6PDM1Pj60QbwndGBtag8M9\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1HbIOrOBcvkF5-rfbwvSkijevvQ0GY8Vb\" width=400px>\n",
        "    <figcaption align = \"center\"><b>t-SNE on Source & Target</b></figcaption>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Discussions : $P \\rightarrow RW$\n",
        "\n",
        "In the table below are reported the results with respect to the MCD without improvement:\n",
        "\n",
        "<center>\n",
        "\n",
        "| | Accuracy | \n",
        "| :---: | :---: |\n",
        "| MCD | 88% |\n",
        "| MCD + Improvement | 89% | \n",
        "\n",
        "</center>\n",
        "\n",
        "Our improvement proposal brought the accuracy higher of 1% than the MCD method without improvement. \n",
        "\n",
        "As we can see from the graph, the general accuracy in all the epochs is higher, the behaviour is the same too. For this reason we can say that adding supervised learning based on the combined output on the features extractor can bring an additional value to the method and also higher performances.\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1JFkTV6aQUuRMeTSeK7H1VYef_zr_ESdP\" width=400px>\n",
        "    <figcaption align = \"center\"><b>Accuracy MCD and MCD + Improvement</b></figcaption>\n",
        "</p>\n",
        "\n",
        "We can see an improvement not only on the accuracy but also in the minimization of the discrepancy during training. Indeed, we can see from the graph below that the discrepancy of the improved MCD is in general lower than the discrepancy of the MCD.\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1XTLO5-bO_iG-pJzd-jQc5AozsZ2WaDRl\" width=400px>\n",
        "    <figcaption align = \"center\"><b>Accuracy MCD and MCD + Improvement</b></figcaption>\n",
        "</p\n",
        "\n",
        "Furthermore, by comparing the confusion matrix of the MCD and MCD + Improvement, we can see that, even though there are still some errors, the classes that have a very low accuracy reach an higher value with the improvement, while the other classes kept the same value. In particular, we can notice that classes with similar object reach both an higher accuarcy and they are not anymore miss-classified. For example backpack and purse: in the MCD the purse class had a very low accuracy and most of the time it was miss-classified as backpack.\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=151JLK_-ohBakq8jzkAId27S4G_Wpog8g\" width=500px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1cKJVp0WDaRR3NnUrAB5DUpoT9axvZZOS\" width=500px>\n",
        "    <figcaption align = \"center\"><b>Confusion Matrix of MCD and MCD + Improvement</b></figcaption>\n",
        "</p>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $RW \\rightarrow P$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHqe64kTPJuV"
      },
      "outputs": [],
      "source": [
        "feature_model = FeatureModel(resnet_version='resnet18')\n",
        "model_s = Predictor(input_size=feature_model.output_size)\n",
        "model_t = Predictor(input_size=feature_model.output_size)\n",
        "\n",
        "opt_f = get_optimizer(feature_model, optimizer='sgd', lr=1e-2)\n",
        "opt_s = get_optimizer(model_s, optimizer='sgd', lr=1e-1)\n",
        "opt_t = get_optimizer(model_t, optimizer='sgd', lr=1e-1)\n",
        "\n",
        "scheduler_f = get_scheduler(opt_f)\n",
        "scheduler_s = get_scheduler(opt_s)\n",
        "scheduler_t = get_scheduler(opt_t)\n",
        "\n",
        "criterion = get_criterion(criterion='cross_entropy')\n",
        "\n",
        "# Initialize wandb\n",
        "experiment_name = 'UDA_RW_P_1'\n",
        "\n",
        "wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=DEFAULT_WANDB_PROJECT_NAME, \n",
        "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    name=experiment_name, \n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        \"feature_model\": feature_model.__class__.__name__,\n",
        "        \"source_model\": model_s.__class__.__name__,\n",
        "        \"target_model\": model_t.__class__.__name__,\n",
        "        \"features_optimizer\": opt_f.__class__.__name__,\n",
        "        \"source_optimizer\": opt_s.__class__.__name__,\n",
        "        \"target_optimizer\": opt_t.__class__.__name__,\n",
        "        \"features_scheduler\": scheduler_f.__class__.__name__,\n",
        "        \"source_scheduler\": scheduler_s.__class__.__name__,\n",
        "        \"target_scheduler\": scheduler_t.__class__.__name__,\n",
        "        \"criterion\": criterion.__class__.__name__,\n",
        "        \"training_type\": \"UDA RW-P Improvement 1\",\n",
        "        \"training_dataset\": \"RealWorld Supervised + Products Unsupervised\",\n",
        "        \"evaluation_dataset\": \"Products\",\n",
        "        \"learning_rate_feature_model\": opt_f.param_groups[0]['lr'],\n",
        "        \"learning_rate_source_model\": opt_s.param_groups[0]['lr'],\n",
        "        \"learning_rate_target_model\": opt_t.param_groups[0]['lr'],\n",
        "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
        "        \"epochs\": 40,\n",
        "    }\n",
        ")\n",
        "\n",
        "accuracy_uda_rw_p_1 = train_mcd(\n",
        "    feature_model=feature_model,\n",
        "    classifiers=(model_s, model_t),\n",
        "    optimizers=(opt_f, opt_s, opt_t),\n",
        "    schedulers=(scheduler_f, scheduler_s, scheduler_t),\n",
        "    criterion=criterion,\n",
        "    train_datasets=(train_loader_reals, train_loader_products),\n",
        "    eval_dataset=test_loader_products,\n",
        "    n_epochs=DEFAULT_EPOCHS + 10,\n",
        "    k=DEFAULT_K_STEPS,\n",
        "    save_name=experiment_name\n",
        ")\n",
        "\n",
        "# Close wandb\n",
        "wandb.finish()\n",
        "\n",
        "# Delete variables\n",
        "del feature_model, model_s, model_t, opt_f, opt_s, opt_t, scheduler_f, scheduler_s, scheduler_t, criterion, experiment_name\n",
        "\n",
        "# Empty cache\n",
        "if DEFAULT_DEVICE.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy UDA RW-P: {:.2f}%\".format(accuracy_uda_rw_p_1*100))\n",
        "print(\"Gain UDA RW-P: {:.2f}%\".format((accuracy_uda_rw_p_1 - accuracy_baseline_rw_p)*100))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Results \n",
        "\n",
        "<center>\n",
        "\n",
        "| | Accuracy | Loss |\n",
        "| :---: | :---: | :---: |\n",
        "| Train source (supervised) | 90.4 % | 0.001 |\n",
        "| Train target (unsupervised) | 84.3 % | 0.001 |\n",
        "| Test source | 95.5 % | 0.001 |\n",
        "| Test target | 95.3 % | 0.001 |\n",
        "| Test overall | 95.8 % | 0.001 |\n",
        "\n",
        "</center>\n",
        "\n",
        "###### Metrics\n",
        "\n",
        "<center>\n",
        "\n",
        "| | precision | recall | f1-score | support |\n",
        "| :---: | :---: | :---: | :---: | :---: |\n",
        "| backpack | 0.88 | 1.00 | 0.94 | 15 |\n",
        "| bookcase | 0.83 | 0.83 | 0.83 | 18 |\n",
        "| car jack | 1.00 | 0.94 | 0.97 | 17 |\n",
        "| comb | 1.00 | 1.00 | 1.00 | 19 |\n",
        "| crown | 0.97 | 1.00 | 0.98 | 29 |\n",
        "| file cabinet | 1.00 | 0.90 | 0.95 | 21 |\n",
        "| flat iron | 1.00 | 1.00 | 1.00 | 20 |\n",
        "| game controller | 0.87 | 0.95 | 0.91 | 21 |\n",
        "| glasses | 0.92 | 0.83 | 0.87 | 29 |\n",
        "| helicopter | 1.00 | 1.00 | 1.00 | 17 |\n",
        "| ice skates | 0.96 | 0.96 | 0.96 | 24 |\n",
        "| letter tray | 0.89 | 1.00 | 0.94 | 17 |\n",
        "| monitor | 0.94 | 1.00 | 0.97 | 16 |\n",
        "| mug | 1.00 | 1.00 | 1.00 | 16 |\n",
        "| network switch | 1.00 | 1.00 | 1.00 | 19 |\n",
        "| over-ear headphones | 0.95 | 0.95 | 0.95 | 19 |\n",
        "| pen | 1.00 | 1.00 | 1.00 | 19 |\n",
        "| purse | 1.00 | 0.96 | 0.98 | 24 |\n",
        "| stand mixer | 0.95 | 0.90 | 0.92 | 20 |\n",
        "| stroller | 1.00 | 1.00 | 1.00 | 20 |\n",
        "| | | | | |\n",
        "| accuracy | | | 0.96 | 400 |\n",
        "| macro avg | 0.96 | 0.96 | 0.96 | 400 |\n",
        "| weighted avg | 0.96 | 0.96 | 0.96 | 400 |\n",
        "\n",
        "</center>\n",
        "\n",
        "###### Loss and Accuracy curves\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1Lcd55AfzshVfhlZycNhdv1yefEkJ5eCV\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1COPj3-mK6UHvYbNv0cXkB4L78pY5AqcO\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1xD1oAdi5k_tYBzM41cExYfXXhhc3Guf3\" width=400px>\n",
        "    <figcaption align = \"center\"><b>Loss & Accuracy & Discrepancy(features only)</b></figcaption>\n",
        "</p>\n",
        "\n",
        "###### Confusion Matrix\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=13jpRypMmlD-C79RJXXsaZQj81p9gfgTm\" width=600px>\n",
        "    <figcaption align = \"center\"><b>Confusion Matrix</b></figcaption>\n",
        "</p>\n",
        "\n",
        "###### t-SNE\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=10iOJqbUNu1XxQk11ZIX73VKRbmrFciTz\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1IWJwGILCo1eG5rLli7UjAOKrw8Sn3BIG\" width=400px>\n",
        "    <figcaption align = \"center\"><b>t-SNE on Source & Target</b></figcaption>\n",
        "</p>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Discussion $RW \\rightarrow P$\n",
        "\n",
        "In the table below are reported the results with respect to the MCD without improvement:\n",
        "\n",
        "<center>\n",
        "\n",
        "| | Accuracy | \n",
        "| :---: | :---: |\n",
        "| MCD | 96.5% |\n",
        "| MCD + Improvement | 95.8% | \n",
        "\n",
        "</center>\n",
        "\n",
        "For this type of task the proposal idea did not bring any improvement as we can see from the accuracy. Even though the accuracy of the MCD was already very very good and it was very difficult to bring something to increase the accuracy. \n",
        "\n",
        "Also for this case we report the graphs for accuracy and discrepancy to show the results. Also in this case the discrepancy is in general lower than the MCD one, but despite of this we did not have any improvement. \n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1iDPtMjod1i7aRI3gy8_kxK3NA-kxugjC\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=1EiBQIrctj_BGaSsA7LWJUplmAFardVaz\" width=400px>\n",
        "    <figcaption align = \"center\"><b>t-SNE on Source & Target</b></figcaption>\n",
        "</p>\n",
        "\n",
        "Even though there are no improvements in this case, the performance are pretty similar and the classes are still well separated. \n",
        "From the confusion matrix we can see that some classes which were missclassified with some others improved their performances, but withthe Improvement started to be missclassified with some others, as we can see for example with game controller and helicopter.\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1Zo2TUDgOj1wd8SF8fdsuUl6bWgBaBGOL\" width=400px>\n",
        "    <img src=\"https://drive.google.com/uc?id=13jpRypMmlD-C79RJXXsaZQj81p9gfgTm\" width=400px>\n",
        "    <figcaption align = \"center\"><b>Confusion Matrix</b></figcaption>\n",
        "</p>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq6h-G_wPJuQ"
      },
      "source": [
        "### Improvement 2 : Loss function\n",
        "\n",
        "In the previous improvement we have seen that the accuracy of the model is improved, but the loss is not much different. We think that the loss is not improved because the loss function is not the best one for this task. \n",
        "\n",
        "In the paper of the MCD method the authors are using the CrossEntropy loss. However, we thought about a better way to compute the loss, which we took from [CDA: Contrastive-adversarial Domain Adaptation](https://arxiv.org/abs/2301.03826).\n",
        "\n",
        "They propose a loss function which is a combination of a two-stage loss that corresponds to the supervised and unsupervised steps. The loss function is defined as:\n",
        " - **Stage 1** : the loss is as a sum of the CrossEntropy loss and a Supervised Contrastive Loss. The Supervised Contrastive Loss ($L_{SupCL}$) is computed as:\n",
        "   $$L_{SupCL}(X_s, Y_s) = - \\sum_{z, z^+ \\in D_s} \\log \\frac{\\exp (z^T z^+ / \\tau)}{\\exp (z^T z^+ / \\tau) + \\sum_{z^- \\in D_s} \\exp (z^T z^- / \\tau)}$$\n",
        "   where variable $z_s$ denote the $l_2$ normalized latent embedding generated by $G$ (feature generator) corresponding to the input sample $x_s$. $D_s$ is the source domain while the target domain is $D_t$. The variable $\\tau$ refers to the temperature scaling (hyperparameter set to `0.1` as default) which affects how the model learns from hard negatives.\n",
        " - **Stage 2** : the loss is a cross-domain contrastive loss:\n",
        "   $$L_{CrossCL}(X_s, Y_s, X_t) = - \\sum_{i = 1 ;\\ z_s \\in D_s ;\\ z_t \\in D_t}^N \\log \\frac{\\exp ({z_s^i}^T z^i_t / \\tau)}{\\exp ({z_s^i}^T z^i_t / \\tau) + \\sum_{i \\neq k = 1}^N \\exp ({z_s^i}^T z^k_t / \\tau)}$$\n",
        "\n",
        "However, we have not been able to implement the second stage of loss function because they assume that the target domain has some form of pseudo-labels. The authors propose to use *k*-means clustering to generate pseudo-labels for the target domain. We did not use it since we can not use labels for the target domain.\n",
        "\n",
        "Thus, the loss function at **stage 2** is implemented as the MCD loss function: CrossEntropy - Discrepancy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuzqFQ49PJuQ"
      },
      "outputs": [],
      "source": [
        "def supervised_contrastive_loss(zs, ys, temperature=100):\n",
        "    \"\"\"\n",
        "    zs is the features output by the resnet\n",
        "    ys is the label (ground truth)\n",
        "    \"\"\"\n",
        "    # Normalize the embeddings\n",
        "    zs = F.normalize(zs, dim=1)\n",
        "    loss = 0\n",
        "    logits_plus = 0\n",
        "    logits_neg = 0\n",
        "    for i in range(DEFAULT_NUM_CLASSES):\n",
        "        # Initialize z+ and z- as:\n",
        "        # z+ = the embeddings of the input with the same class as the current input (ground truth class is obtained from the ground truth and class predicted from xs)\n",
        "        # z- = the embeddings of the input with a different class as the current input (ground truth class is obtained from the ground truth and class predicted from xs)\n",
        "        z_plus = zs[ys == i]\n",
        "        z_minus = zs[ys != i]\n",
        "        for z in z_plus:\n",
        "          logits_plus = torch.exp(torch.matmul(zs, z.t()) / temperature)\n",
        "          \n",
        "          for m in z_minus:\n",
        "            logits_neg += torch.exp(torch.matmul(zs, m.t())/temperature)\n",
        "\n",
        "        # Compute the loss for the positive pairs\n",
        "        loss += torch.log(logits_plus / (logits_plus + logits_neg))\n",
        "        \n",
        "    return -loss.mean() #/ 1000\n",
        "\n",
        "def loss_stage_1(source_x, source_y, target_x, features, temperature: float = 0.1):\n",
        "    # First get the loss with CrossEntropyLoss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss = criterion(source_x, source_y) + criterion(target_x, source_y)\n",
        "\n",
        "    # Then add the loss with the supervised contrastive loss\n",
        "    return loss + supervised_contrastive_loss(features, source_y, temperature)\n",
        "\n",
        "def loss_stage_2(source_x, source_y, target_x, features, temperature: float = 0.1):\n",
        "    # First get the loss with CrossEntropyLoss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss = criterion(source_x, source_y) + criterion(target_x, source_y)\n",
        "\n",
        "    # Compute discrepancy loss\n",
        "    discrepancy_loss = discrepancy(source_x, target_x)\n",
        "\n",
        "    # Then remove the discrepancy loss\n",
        "    return loss - discrepancy_loss, discrepancy_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKsOzm0-PJuQ"
      },
      "source": [
        "#### Train & Test functions\n",
        "\n",
        "This functions have to be redefined in order to use the new loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GK6TxscPJuR"
      },
      "outputs": [],
      "source": [
        "def train_improved_loss_one_step(\n",
        "        features_model: nn.Module, \n",
        "        classifiers: Tuple[nn.Module, nn.Module],\n",
        "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.Optimizer, torch.optim.Optimizer], \n",
        "        datasets: Tuple[DataLoader, DataLoader],\n",
        "        k: int = DEFAULT_K_STEPS,\n",
        "        device: str = DEFAULT_DEVICE,\n",
        "    ) -> Tuple[nn.Module, nn.Module, nn.Module]:\n",
        "    # Extract classifiers\n",
        "    classifier_source, classifier_target = classifiers\n",
        "\n",
        "    # Extract optimizers\n",
        "    optimizer_features, optimizer_source, optimizer_target = optimizers\n",
        "\n",
        "    # Extract datasets\n",
        "    train_loader_source, train_loader_target = datasets\n",
        "\n",
        "    # Set models to train mode\n",
        "    features_model.train()\n",
        "    classifier_source.train()\n",
        "    classifier_target.train()\n",
        "    \n",
        "    # Initialize cumulative variables\n",
        "    cum_loss_source = 0\n",
        "    cum_loss_target = 0\n",
        "    cum_accuracy_source = 0\n",
        "    cum_accuracy_target = 0\n",
        "    n_samples = 0\n",
        "\n",
        "    # Train loop    \n",
        "    for (x_source, y_source), (x_target, y_target) in zip(train_loader_source, train_loader_target):\n",
        "        # Move to device\n",
        "        x_source, y_source = x_source.to(device), y_source.to(device)\n",
        "        x_target, y_target = x_target.to(device), y_target.to(device)\n",
        "\n",
        "        # Extract features and compute output\n",
        "        features_source = features_model(x_source)\n",
        "        out_source = classifier_source(features_source)\n",
        "        out_target = classifier_target(features_source)\n",
        "\n",
        "        # Compute loss\n",
        "        loss_s = loss_stage_1(out_source, y_source, out_target, features_source)\n",
        "        cum_loss_source += loss_s.item()\n",
        "\n",
        "        # Compute accuracy\n",
        "        cum_accuracy_source += compute_accuracy(out_source, y_source)\n",
        "        cum_accuracy_source += compute_accuracy(out_target, y_source)\n",
        "\n",
        "        # Set optimizers to zero_grad\n",
        "        optimizer_features.zero_grad()\n",
        "        optimizer_source.zero_grad()\n",
        "        optimizer_target.zero_grad()\n",
        "\n",
        "        # Backward\n",
        "        loss_s.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer_features.step()\n",
        "        optimizer_source.step()\n",
        "        optimizer_target.step()\n",
        "\n",
        "        ### UDA ###\n",
        "        # Extract features and compute output - source\n",
        "        features_source = features_model(x_source)\n",
        "        out_source_s = classifier_source(features_source)\n",
        "        out_target_s = classifier_target(features_source)\n",
        "\n",
        "        # Extract features and compute output - target\n",
        "        features_target = features_model(x_target)\n",
        "        out_source_t = classifier_source(features_target)\n",
        "        out_target_t = classifier_target(features_target)\n",
        "        \n",
        "        # Compute loss - Only on source!!!\n",
        "        loss, d = loss_stage_2(out_source_s, y_source, out_target_s, features_source)\n",
        "        cum_loss_target += loss.item()\n",
        "\n",
        "        # Compute accuracy - Can we compute accuracy on target since it is unsupervised? \n",
        "        cum_accuracy_target += compute_accuracy(out_source_t, y_target)\n",
        "        cum_accuracy_target += compute_accuracy(out_target_t, y_target)\n",
        "\n",
        "        # Backward the toal loss of source and target\n",
        "        loss.backward()\n",
        "\n",
        "        # Set optimizers to zero_grad\n",
        "        optimizer_source.zero_grad()\n",
        "        optimizer_target.zero_grad()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer_source.step()\n",
        "        optimizer_target.step()\n",
        "        \n",
        "        # Update n_samples\n",
        "        n_samples += x_source.shape[0]\n",
        "        n_samples += x_target.shape[0]\n",
        "        \n",
        "        for i in range(k):\n",
        "            # Extract features and compute output \n",
        "            features_target = features_model(x_target)\n",
        "            out_source_t = classifier_source(features_target)\n",
        "            out_target_t = classifier_target(features_target)\n",
        "\n",
        "            # Compute loss\n",
        "            loss_discrepancy = discrepancy(out_source_t, out_target_t)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer_features.zero_grad()\n",
        "\n",
        "            # Backpropagate\n",
        "            loss_discrepancy.backward()\n",
        "            \n",
        "            # Step optimizer\n",
        "            optimizer_features.step()\n",
        "\n",
        "\n",
        "    return {\n",
        "        'train/discrepancy' : d,\n",
        "        'train/loss_features' : loss_discrepancy,\n",
        "        'train/loss_source': cum_loss_source / n_samples,\n",
        "        'train/loss_target': cum_loss_target / n_samples,\n",
        "        'train/accuracy_source': cum_accuracy_source / n_samples,\n",
        "        'train/accuracy_target': cum_accuracy_target / n_samples,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkCuuugVPJuR"
      },
      "outputs": [],
      "source": [
        "def train_improved(\n",
        "        feature_model: nn.Module, \n",
        "        classifiers: Tuple[nn.Module, nn.Module],\n",
        "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.Optimizer, torch.optim.Optimizer], \n",
        "        schedulers: Tuple[torch.optim.lr_scheduler._LRScheduler, torch.optim.lr_scheduler._LRScheduler, torch.optim.lr_scheduler._LRScheduler],\n",
        "        train_datasets: Tuple[DataLoader, DataLoader],\n",
        "        eval_dataset: DataLoader,\n",
        "        n_epochs: int = DEFAULT_EPOCHS,\n",
        "        k: int = DEFAULT_K_STEPS,\n",
        "        device: str = DEFAULT_DEVICE,\n",
        "        save_name: str = \"UDA_MCD_P_RW\",\n",
        "    ) -> Tuple[nn.Module, nn.Module, nn.Module]:\n",
        "    # Extract classifiers\n",
        "    classifier_source, classifier_target = classifiers\n",
        "\n",
        "    # Move to device\n",
        "    feature_model.to(device)\n",
        "    classifier_source.to(device)\n",
        "    classifier_target.to(device)\n",
        "\n",
        "    # Extract schedulers\n",
        "    scheduler_features, scheduler_source, scheduler_target = schedulers\n",
        "\n",
        "    # Initialize variables\n",
        "    best_accuracy = 0\n",
        "    best_model_f = None\n",
        "    best_model_s = None\n",
        "    best_model_t = None\n",
        "\n",
        "    # Start timer\n",
        "    start_timer = time.time()\n",
        "\n",
        "    # Train loop\n",
        "    for epoch in range(n_epochs):\n",
        "        # Train\n",
        "        train_metrics = train_improved_loss_one_step(\n",
        "            features_model=feature_model, \n",
        "            classifiers=classifiers, \n",
        "            optimizers=optimizers, \n",
        "            datasets=train_datasets, \n",
        "            k=k,\n",
        "            device=device,\n",
        "        )\n",
        "        # Test\n",
        "        test_metrics = test_mcd_one_step(\n",
        "            feature_model=feature_model, \n",
        "            classifiers=classifiers, \n",
        "            dataset=eval_dataset, \n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        # Put together metrics\n",
        "        metrics = {**train_metrics, **test_metrics}\n",
        "\n",
        "        # Save best model\n",
        "        if test_metrics['eval/accuracy'] > best_accuracy:\n",
        "            best_accuracy = test_metrics['eval/accuracy']\n",
        "\n",
        "            best_model_f = copy.deepcopy(feature_model)\n",
        "            best_model_s = copy.deepcopy(classifier_source)\n",
        "            best_model_t = copy.deepcopy(classifier_target)\n",
        "            \n",
        "            torch.save(best_model_f.state_dict(), os.path.join(LOG_PATH_MODELS, save_name+\"_features_extractor.pth\"))\n",
        "            torch.save(best_model_s.state_dict(), os.path.join(LOG_PATH_MODELS, save_name+\"_source.pth\"))\n",
        "            torch.save(best_model_t.state_dict(), os.path.join(LOG_PATH_MODELS, save_name+\"_target.pth\"))\n",
        "\n",
        "        # Log metrics\n",
        "        opt_f, opt_s, opt_t = optimizers\n",
        "        make_log_print(\"Train\", (epoch+1, n_epochs), time.time() - start_timer, metrics, lr=(round(opt_f.param_groups[0]['lr'], 6), round(opt_s.param_groups[0]['lr'], 6), round(opt_t.param_groups[0]['lr'], 6)))\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler_features.step()\n",
        "        scheduler_source.step()\n",
        "        scheduler_target.step()\n",
        "\n",
        "    visualize_results\n",
        "\n",
        "    return best_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dhnpcKFPJuR"
      },
      "source": [
        "#### $P \\rightarrow RW$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNTMBHbePJuS"
      },
      "outputs": [],
      "source": [
        "feature_model = FeatureModel(resnet_version='resnet18')\n",
        "model_s = Predictor(input_size=feature_model.output_size)\n",
        "model_t = Predictor(input_size=feature_model.output_size)\n",
        "\n",
        "opt_f = get_optimizer(feature_model, optimizer='sgd', lr=1e-2)\n",
        "opt_s = get_optimizer(model_s, optimizer='sgd', lr=1e-1)\n",
        "opt_t = get_optimizer(model_t, optimizer='sgd', lr=1e-1)\n",
        "\n",
        "scheduler_f = get_scheduler(opt_f)\n",
        "scheduler_s = get_scheduler(opt_s)\n",
        "scheduler_t = get_scheduler(opt_t)\n",
        "\n",
        "# Initialize wandb\n",
        "experiment_name = 'Improvment_Loss_P_RW'\n",
        "\n",
        "wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=DEFAULT_WANDB_PROJECT_NAME, \n",
        "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    name=experiment_name, \n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        \"feature_model\": feature_model.__class__.__name__,\n",
        "        \"source_model\": model_s.__class__.__name__,\n",
        "        \"target_model\": model_t.__class__.__name__,\n",
        "        \"features_optimizer\": opt_f.__class__.__name__,\n",
        "        \"source_optimizer\": opt_s.__class__.__name__,\n",
        "        \"target_optimizer\": opt_t.__class__.__name__,\n",
        "        \"features_scheduler\": scheduler_f.__class__.__name__,\n",
        "        \"source_scheduler\": scheduler_s.__class__.__name__,\n",
        "        \"target_scheduler\": scheduler_t.__class__.__name__,\n",
        "        \"criterion\": \"2-Stage Loss\",\n",
        "        \"training_type\": \"UDA P-RW Improved Loss\",\n",
        "        \"training_dataset\": \"Products Supervised + RealWorld Unsupervised\",\n",
        "        \"evaluation_dataset\": \"RealWorld\",\n",
        "        \"learning_rate_feature_model\": opt_f.param_groups[0]['lr'],\n",
        "        \"learning_rate_source_model\": opt_s.param_groups[0]['lr'],\n",
        "        \"learning_rate_target_model\": opt_t.param_groups[0]['lr'],\n",
        "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
        "        \"epochs\": DEFAULT_EPOCHS,\n",
        "    }\n",
        ")\n",
        "\n",
        "accuracy_improvement_loss_p_rw = train_mcd(\n",
        "    feature_model=feature_model,\n",
        "    classifiers=(model_s, model_t),\n",
        "    optimizers=(opt_f, opt_s, opt_t),\n",
        "    schedulers=(scheduler_f, scheduler_s, scheduler_t),\n",
        "    train_datasets=(train_loader_products, train_loader_reals),\n",
        "    eval_dataset=test_loader_reals,\n",
        "    n_epochs=DEFAULT_EPOCHS,\n",
        "    k=DEFAULT_K_STEPS,\n",
        "    save_name=experiment_name\n",
        ")\n",
        "\n",
        "# Close wandb\n",
        "wandb.finish()\n",
        "\n",
        "# Delete variables\n",
        "del feature_model, model_s, model_t, opt_f, opt_s, opt_t, scheduler_f, scheduler_s, scheduler_t, criterion, experiment_name\n",
        "\n",
        "# Empty cache\n",
        "if DEFAULT_DEVICE.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy UDA P-RW: {:.2f}%\".format(accuracy_improvement_loss_p_rw*100))\n",
        "print(\"Gain UDA P-RW: {:.2f}%\".format((accuracy_improvement_loss_p_rw - accuracy_baseline_p_rw)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU8ataFdPJuS"
      },
      "source": [
        "#### $RW \\rightarrow P$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUgL8qFcPJuS"
      },
      "outputs": [],
      "source": [
        "feature_model = FeatureModel(resnet_version='resnet18')\n",
        "model_s = Predictor(input_size=feature_model.output_size)\n",
        "model_t = Predictor(input_size=feature_model.output_size)\n",
        "\n",
        "opt_f = get_optimizer(feature_model, optimizer='sgd', lr=1e-2)\n",
        "opt_s = get_optimizer(model_s, optimizer='sgd', lr=1e-1)\n",
        "opt_t = get_optimizer(model_t, optimizer='sgd', lr=1e-1)\n",
        "\n",
        "scheduler_f = get_scheduler(opt_f)\n",
        "scheduler_s = get_scheduler(opt_s)\n",
        "scheduler_t = get_scheduler(opt_t)\n",
        "\n",
        "# Initialize wandb\n",
        "experiment_name = 'Improvment_Loss_RW_P'\n",
        "\n",
        "wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=DEFAULT_WANDB_PROJECT_NAME, \n",
        "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    name=experiment_name, \n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "        \"feature_model\": feature_model.__class__.__name__,\n",
        "        \"source_model\": model_s.__class__.__name__,\n",
        "        \"target_model\": model_t.__class__.__name__,\n",
        "        \"features_optimizer\": opt_f.__class__.__name__,\n",
        "        \"source_optimizer\": opt_s.__class__.__name__,\n",
        "        \"target_optimizer\": opt_t.__class__.__name__,\n",
        "        \"features_scheduler\": scheduler_f.__class__.__name__,\n",
        "        \"source_scheduler\": scheduler_s.__class__.__name__,\n",
        "        \"target_scheduler\": scheduler_t.__class__.__name__,\n",
        "        \"criterion\": \"2-Stage Loss\",\n",
        "        \"training_type\": \"UDA RW-P Improved Loss\",\n",
        "        \"training_dataset\": \"RealWorld Supervised + Products Unsupervised\",\n",
        "        \"evaluation_dataset\": \"Products\",\n",
        "        \"learning_rate_feature_model\": opt_f.param_groups[0]['lr'],\n",
        "        \"learning_rate_source_model\": opt_s.param_groups[0]['lr'],\n",
        "        \"learning_rate_target_model\": opt_t.param_groups[0]['lr'],\n",
        "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
        "        \"epochs\": DEFAULT_EPOCHS,\n",
        "    }\n",
        ")\n",
        "\n",
        "accuracy_improvement_loss_rw_p = train_mcd(\n",
        "    feature_model=feature_model,\n",
        "    classifiers=(model_s, model_t),\n",
        "    optimizers=(opt_f, opt_s, opt_t),\n",
        "    schedulers=(scheduler_f, scheduler_s, scheduler_t),\n",
        "    train_datasets=(train_loader_reals, train_loader_products),\n",
        "    eval_dataset=test_loader_products,\n",
        "    n_epochs=DEFAULT_EPOCHS,\n",
        "    k=DEFAULT_K_STEPS,\n",
        "    save_name=experiment_name\n",
        ")\n",
        "\n",
        "# Close wandb\n",
        "wandb.finish()\n",
        "\n",
        "# Delete variables\n",
        "del feature_model, model_s, model_t, opt_f, opt_s, opt_t, scheduler_f, scheduler_s, scheduler_t, experiment_name\n",
        "\n",
        "# Empty cache\n",
        "if DEFAULT_DEVICE.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy UDA P-RW: {:.2f}%\".format(accuracy_improvement_loss_rw_p*100))\n",
        "print(\"Gain UDA P-RW: {:.2f}%\".format((accuracy_improvement_loss_rw_p - accuracy_baseline_p_rw)*100))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Discussion\n",
        "\n",
        "Unfortunately, on both directions the results obtained were not as good as we expected. The accuracy of the model is not improved, it is even worse than the baseline. Indeed, the accuracy of the model starts around 5% which is not good. Moreover, during the epochs the accuracy is not improving, it freezes at 5% and the loss is not decreasing.\n",
        "\n",
        "We think that a crucial factor is that we have not been able to implement the second stage of the loss function. Indeed, the authors propose to use *k*-means clustering to generate pseudo-labels for the target domain. We did not use it since we can not use labels for the target domain.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final discussion\n",
        "\n",
        "In this project we have implemented the MCD method and we have tried to improve it. \n",
        "\n",
        "We started from a baseline that was pretty good (80.7% for the $P \\rightarrow RW$ direction and 92.2% for the $RW \\rightarrow P$ direction). But we had the margin to improve it since the upperbounds were 9.5% and 5.1% better than the baseline. \n",
        "\n",
        "With the MCD method we have obtained a good accuracy, with the $P \\rightarrow RW$ direction we have obtained an accuracy of 88.0% with some margin for other improvements (2.5%) while with the $RW \\rightarrow P$ direction we have obtained an accuracy of 96.5% with very little margin for improvements (0.8%).\n",
        "\n",
        "We tried to improve the MCD method with an idea that we had and then by changing the loss function with the one proposed in [CDA: Contrastive-adversarial Domain Adaptation](https://arxiv.org/abs/2301.03826). With the first of the two improvements we have obtained a better accuracy for the $P \\rightarrow RW$ direction, while with the second one we have not obtained any improvement, it even got worse.\n",
        "\n",
        "The overall results are reported in the table below:\n",
        "\n",
        "<center>\n",
        "\n",
        "| Direction | Stage | Accuracy |\n",
        "| :---: | :---: | :---: |\n",
        "| $P \\rightarrow RW$ | Baseline | 80.7% |\n",
        "| $P \\rightarrow RW$ | MCD | 88.0 % |\n",
        "| $P \\rightarrow RW$ | Improvement 1 | 89.0% |\n",
        "| $P \\rightarrow RW$ | Upper bound | 90.5% |\n",
        "| | | |\n",
        "| $RW \\rightarrow P$ | Baseline | 92.2% |\n",
        "| $RW \\rightarrow P$ | MCD | 96.5% |\n",
        "| $RW \\rightarrow P$ | Improvement 1 | 95.8% |\n",
        "| $RW \\rightarrow P$ | Upper bound | 97.3% |\n",
        "\n",
        "</center>\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "In conclusion, we have implemented the MCD method and we have tried to improve it. We have obtained very good results both on $P \\rightarrow RW$ direction and $RW \\rightarrow P$. The proposed idea, improved only the result for the first direction, while for the second one reached somehow the same accuracy, without bringing any improvement. \n",
        "\n",
        "Finally we can say that MCD works already very well, the proposed idea is a plus, especially for those cases that are more difficult to work with."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10 (tags/v3.9.10:f2f3f53, Jan 17 2022, 15:14:21) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "d45554ccdb057191d4a4d227da3b5b2ffbe1c21733e8fa042a058a88b8e87cec"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "019f5cab445e450aae3898e9531bb65b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7be1f2d67d5402ebef40a0411d73cdb",
              "IPY_MODEL_cc99f98bea624e1cb9d9a0c4762587e6"
            ],
            "layout": "IPY_MODEL_624f6cd693a64210a6a5db96a7fbc91d"
          }
        },
        "1e9a6a9e99f94ed5bf3e2eff7dfde74b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "624f6cd693a64210a6a5db96a7fbc91d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6737094c2c284cc18b239e240bac065a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e9a6a9e99f94ed5bf3e2eff7dfde74b",
            "placeholder": "​",
            "style": "IPY_MODEL_bffc5f8f29e54feb833fdf6a1f4d1560",
            "value": "0.001 MB of 0.013 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "812d66b3989648bab09a13768f20dfde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "892e357dd136460f9c627b523dba9e91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aad5e9bbbc464e1387e2ab20c6109011": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5ad309825c846daa01837eed020277c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be32e6696f184b88980f5b9b0211b949": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6737094c2c284cc18b239e240bac065a",
              "IPY_MODEL_f8e67a17714240409d0406779f75b4fc"
            ],
            "layout": "IPY_MODEL_812d66b3989648bab09a13768f20dfde"
          }
        },
        "bffc5f8f29e54feb833fdf6a1f4d1560": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c20caf69e0984953aa64a2640ab7f033": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7be1f2d67d5402ebef40a0411d73cdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4bda0833c334b509717c01f68260bcf",
            "placeholder": "​",
            "style": "IPY_MODEL_c20caf69e0984953aa64a2640ab7f033",
            "value": "0.013 MB of 0.013 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "cc99f98bea624e1cb9d9a0c4762587e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aad5e9bbbc464e1387e2ab20c6109011",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5ad309825c846daa01837eed020277c",
            "value": 1
          }
        },
        "d4bda0833c334b509717c01f68260bcf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8e67a17714240409d0406779f75b4fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_892e357dd136460f9c627b523dba9e91",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd5556e9746941e9b4461e0bfa36d38f",
            "value": 0.05282938590270884
          }
        },
        "fd5556e9746941e9b4461e0bfa36d38f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
